{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ed3051d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "from torch.nn import *\n",
    "\n",
    "import torchvision\n",
    "from torchvision.transforms import *\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4122f6ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [2, 64, 224, 224]           9,472\n",
      "         LeakyReLU-2          [2, 64, 224, 224]               0\n",
      "          YoloConv-3          [2, 64, 224, 224]               0\n",
      "         MaxPool2d-4          [2, 64, 112, 112]               0\n",
      "         YoloLayer-5          [2, 64, 112, 112]               0\n",
      "            Conv2d-6         [2, 192, 112, 112]         110,784\n",
      "         LeakyReLU-7         [2, 192, 112, 112]               0\n",
      "          YoloConv-8         [2, 192, 112, 112]               0\n",
      "         MaxPool2d-9           [2, 192, 56, 56]               0\n",
      "        YoloLayer-10           [2, 192, 56, 56]               0\n",
      "           Conv2d-11           [2, 128, 56, 56]          24,704\n",
      "        LeakyReLU-12           [2, 128, 56, 56]               0\n",
      "         YoloConv-13           [2, 128, 56, 56]               0\n",
      "           Conv2d-14           [2, 256, 56, 56]         295,168\n",
      "        LeakyReLU-15           [2, 256, 56, 56]               0\n",
      "         YoloConv-16           [2, 256, 56, 56]               0\n",
      "           Conv2d-17           [2, 256, 56, 56]          65,792\n",
      "        LeakyReLU-18           [2, 256, 56, 56]               0\n",
      "         YoloConv-19           [2, 256, 56, 56]               0\n",
      "           Conv2d-20           [2, 512, 56, 56]       1,180,160\n",
      "        LeakyReLU-21           [2, 512, 56, 56]               0\n",
      "         YoloConv-22           [2, 512, 56, 56]               0\n",
      "        MaxPool2d-23           [2, 512, 28, 28]               0\n",
      "        YoloLayer-24           [2, 512, 28, 28]               0\n",
      "           Conv2d-25           [2, 256, 28, 28]         131,328\n",
      "        LeakyReLU-26           [2, 256, 28, 28]               0\n",
      "         YoloConv-27           [2, 256, 28, 28]               0\n",
      "           Conv2d-28           [2, 512, 28, 28]       1,180,160\n",
      "        LeakyReLU-29           [2, 512, 28, 28]               0\n",
      "         YoloConv-30           [2, 512, 28, 28]               0\n",
      "           Conv2d-31           [2, 256, 28, 28]         131,328\n",
      "        LeakyReLU-32           [2, 256, 28, 28]               0\n",
      "         YoloConv-33           [2, 256, 28, 28]               0\n",
      "           Conv2d-34           [2, 512, 28, 28]       1,180,160\n",
      "        LeakyReLU-35           [2, 512, 28, 28]               0\n",
      "         YoloConv-36           [2, 512, 28, 28]               0\n",
      "           Conv2d-37           [2, 256, 28, 28]         131,328\n",
      "        LeakyReLU-38           [2, 256, 28, 28]               0\n",
      "         YoloConv-39           [2, 256, 28, 28]               0\n",
      "           Conv2d-40           [2, 512, 28, 28]       1,180,160\n",
      "        LeakyReLU-41           [2, 512, 28, 28]               0\n",
      "         YoloConv-42           [2, 512, 28, 28]               0\n",
      "           Conv2d-43           [2, 256, 28, 28]         131,328\n",
      "        LeakyReLU-44           [2, 256, 28, 28]               0\n",
      "         YoloConv-45           [2, 256, 28, 28]               0\n",
      "           Conv2d-46           [2, 512, 28, 28]       1,180,160\n",
      "        LeakyReLU-47           [2, 512, 28, 28]               0\n",
      "         YoloConv-48           [2, 512, 28, 28]               0\n",
      "           Conv2d-49           [2, 512, 28, 28]         262,656\n",
      "        LeakyReLU-50           [2, 512, 28, 28]               0\n",
      "         YoloConv-51           [2, 512, 28, 28]               0\n",
      "           Conv2d-52          [2, 1024, 28, 28]       4,719,616\n",
      "        LeakyReLU-53          [2, 1024, 28, 28]               0\n",
      "         YoloConv-54          [2, 1024, 28, 28]               0\n",
      "        MaxPool2d-55          [2, 1024, 14, 14]               0\n",
      "        YoloLayer-56          [2, 1024, 14, 14]               0\n",
      "           Conv2d-57           [2, 512, 14, 14]         524,800\n",
      "        LeakyReLU-58           [2, 512, 14, 14]               0\n",
      "         YoloConv-59           [2, 512, 14, 14]               0\n",
      "           Conv2d-60          [2, 1024, 14, 14]       4,719,616\n",
      "        LeakyReLU-61          [2, 1024, 14, 14]               0\n",
      "         YoloConv-62          [2, 1024, 14, 14]               0\n",
      "           Conv2d-63           [2, 512, 14, 14]         524,800\n",
      "        LeakyReLU-64           [2, 512, 14, 14]               0\n",
      "         YoloConv-65           [2, 512, 14, 14]               0\n",
      "           Conv2d-66          [2, 1024, 14, 14]       4,719,616\n",
      "        LeakyReLU-67          [2, 1024, 14, 14]               0\n",
      "         YoloConv-68          [2, 1024, 14, 14]               0\n",
      "         Identity-69          [2, 1024, 14, 14]               0\n",
      "        YoloLayer-70          [2, 1024, 14, 14]               0\n",
      "YoloFeatureExtractor-71          [2, 1024, 14, 14]               0\n",
      "           Conv2d-72          [2, 1024, 14, 14]       9,438,208\n",
      "        LeakyReLU-73          [2, 1024, 14, 14]               0\n",
      "         YoloConv-74          [2, 1024, 14, 14]               0\n",
      "           Conv2d-75            [2, 1024, 7, 7]       9,438,208\n",
      "        LeakyReLU-76            [2, 1024, 7, 7]               0\n",
      "         YoloConv-77            [2, 1024, 7, 7]               0\n",
      "         Identity-78            [2, 1024, 7, 7]               0\n",
      "        YoloLayer-79            [2, 1024, 7, 7]               0\n",
      "           Conv2d-80            [2, 1024, 7, 7]       9,438,208\n",
      "        LeakyReLU-81            [2, 1024, 7, 7]               0\n",
      "         YoloConv-82            [2, 1024, 7, 7]               0\n",
      "           Conv2d-83            [2, 1024, 7, 7]       9,438,208\n",
      "        LeakyReLU-84            [2, 1024, 7, 7]               0\n",
      "         YoloConv-85            [2, 1024, 7, 7]               0\n",
      "         Identity-86            [2, 1024, 7, 7]               0\n",
      "        YoloLayer-87            [2, 1024, 7, 7]               0\n",
      "           Linear-88                  [2, 4096]     205,524,992\n",
      "        LeakyReLU-89                  [2, 4096]               0\n",
      "          Dropout-90                  [2, 4096]               0\n",
      "           Linear-91                  [2, 1470]       6,022,590\n",
      "   YoloFinalLayer-92              [2, 30, 7, 7]               0\n",
      "   YoloClassifier-93              [2, 30, 7, 7]               0\n",
      "================================================================\n",
      "Total params: 271,703,550\n",
      "Trainable params: 271,703,550\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 4.59\n",
      "Forward/backward pass size (MB): 705.40\n",
      "Params size (MB): 1036.47\n",
      "Estimated Total Size (MB): 1746.46\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "class YoloLayer(Module):\n",
    "    def __init__(self, layer_params, pool=True):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for in_channels, out_channels, kernel_size, stride, activation in layer_params:\n",
    "            layers.append(\n",
    "                YoloConv(in_channels=in_channels, out_channels=out_channels,\n",
    "                         kernel_size=kernel_size, stride=stride, activation=activation)\n",
    "            )\n",
    "        \n",
    "        self.in_layers = Sequential(*layers)\n",
    "        if pool:\n",
    "            self.pool = MaxPool2d(kernel_size=(2, 2), stride=2)\n",
    "        else:\n",
    "            self.pool = Identity()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.in_layers(x)\n",
    "        x = self.pool(x)\n",
    "        return x\n",
    "\n",
    "class YoloConv(Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=(3, 3), stride=1, activation='relu'):\n",
    "        super().__init__()\n",
    "        self.conv = Conv2d(kernel_size=kernel_size, in_channels=in_channels, out_channels=out_channels,\n",
    "                          stride=stride, padding=kernel_size[0] // 2)\n",
    "        if activation.lower() == 'relu':\n",
    "            self.activation = LeakyReLU(negative_slope=0.1, inplace=True)\n",
    "        else:\n",
    "            self.activation = Identity\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        self.activation(x)  # inplace\n",
    "        return x\n",
    "    \n",
    "class YoloFinalLayer(Module):\n",
    "    def __init__(self, activation='relu'):\n",
    "        super().__init__()\n",
    "        self.classifier0 = Linear(in_features=50176, out_features=4096)\n",
    "        if activation.lower() == 'relu':\n",
    "            self.act = LeakyReLU(negative_slope=0.1, inplace=True)\n",
    "        else:\n",
    "            self.act = Identity()\n",
    "        self.dropout = Dropout(0.5)\n",
    "        self.classifier1 = Linear(in_features=4096, out_features=1470)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 50176)\n",
    "        x = self.classifier0(x)\n",
    "        self.act(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.classifier1(x)\n",
    "        x = x.view(-1, 30, 7, 7)\n",
    "        return x\n",
    "    \n",
    "class YoloFeatureExtractor(Module):\n",
    "    def __init__(self, activation='relu'):\n",
    "        super().__init__()\n",
    "        layers = [\n",
    "            YoloLayer([\n",
    "                # in, out, kernel, stride, apoolct\n",
    "                (3, 64, (7, 7), 2, activation),\n",
    "            ]),\n",
    "            YoloLayer([\n",
    "                (64, 192, (3, 3), 1, activation),\n",
    "            ]),\n",
    "            YoloLayer([\n",
    "                (192, 128, (1, 1), 1, activation),\n",
    "                (128, 256, (3, 3), 1, activation),\n",
    "                (256, 256, (1, 1), 1, activation),\n",
    "                (256, 512, (3, 3), 1, activation),\n",
    "            ]),\n",
    "            YoloLayer([\n",
    "                (512, 256, (1, 1), 1, activation),\n",
    "                (256, 512, (3, 3), 1, activation),\n",
    "                (512, 256, (1, 1), 1, activation),\n",
    "                (256, 512, (3, 3), 1, activation),\n",
    "                (512, 256, (1, 1), 1, activation),\n",
    "                (256, 512, (3, 3), 1, activation),\n",
    "                (512, 256, (1, 1), 1, activation),\n",
    "                (256, 512, (3, 3), 1, activation),\n",
    "                (512, 512, (1, 1), 1, activation),\n",
    "                (512, 1024, (3, 3), 1, activation),\n",
    "            ]),\n",
    "            YoloLayer([\n",
    "                (1024, 512, (1, 1), 1, activation),\n",
    "                (512, 1024, (3, 3), 1, activation),\n",
    "                (1024, 512, (1, 1), 1, activation),\n",
    "                (512, 1024, (3, 3), 1, activation),\n",
    "            ], pool=False)\n",
    "        ]\n",
    "        \n",
    "        self.layers = Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        return x\n",
    "            \n",
    "class YoloClassifier(Module):\n",
    "    def __init__(self, activation='relu'):\n",
    "        super().__init__()\n",
    "        layers = [\n",
    "            YoloLayer([\n",
    "                (1024, 1024, (3, 3), 1, 'relu'),\n",
    "                (1024, 1024, (3, 3), 2, 'relu'),\n",
    "            ], pool=False),\n",
    "            YoloLayer([\n",
    "                (1024, 1024, (3, 3), 1, 'relu'),\n",
    "                (1024, 1024, (3, 3), 1, 'relu'),\n",
    "            ], pool=False),\n",
    "            YoloFinalLayer('relu')\n",
    "        ]\n",
    "        \n",
    "        self.layers = Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        return x\n",
    "        \n",
    "\n",
    "class YOLOv1(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.features = YoloFeatureExtractor('relu')\n",
    "        self.classifiers = YoloClassifier('relu')\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifiers(x)\n",
    "        return x\n",
    "    \n",
    "class YOLOv1Pretrainer(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.features = YoloFeatureExtractor('relu')\n",
    "        self.classifiers = Sequential(\n",
    "            AvgPool2d(kernel_size=(7, 7)),\n",
    "            Flatten(),\n",
    "            Linear(in_features=1024, out_features=1000)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifiers(x)\n",
    "        return x\n",
    "\n",
    "model = YOLOv1()\n",
    "summary(model, input_size=(3, 448, 448), batch_size=2, device='cpu')\n",
    "# model = YOLOv1Pretrainer()\n",
    "# summary(model, input_size=(3, 224, 224), batch_size=2, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac3a4fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from tqdm.auto import tqdm\n",
    "from xml.etree import ElementTree\n",
    "\n",
    "import torch\n",
    "import os\n",
    "import math\n",
    "\n",
    "class VOCYOLOAnnotator(object):\n",
    "    def __init__(self, annotation_root, image_root, model_cells=7, extension='jpg'):\n",
    "        super().__init__()\n",
    "        self.annotation_root = annotation_root\n",
    "        self.image_root = image_root\n",
    "        self.model_cells = model_cells\n",
    "        \n",
    "        self.annotation_files = glob(os.path.join(annotation_root, '*.xml'))\n",
    "        self.labels = self.find_object_names()\n",
    "        \n",
    "    def find_object_names(self):\n",
    "        object_map = {}\n",
    "        for xml_filename in tqdm(self.annotation_files, desc='Annotation 내 Object Names 검색'):\n",
    "            with open(xml_filename, 'r') as f:\n",
    "                root = ElementTree.fromstring(f.read())\n",
    "                for item in root.findall('object'):\n",
    "                    object_name = item.find('name').text.strip()\n",
    "                    object_map[object_name] = 1\n",
    "        return list(sorted(object_map.keys()))\n",
    "    \n",
    "    def parse_annotation(self):\n",
    "        annotations = []\n",
    "        for xml_filename in tqdm(self.annotation_files,  desc='Annotation 검색'):\n",
    "            with open(xml_filename, 'r') as f:\n",
    "                root = ElementTree.fromstring(f.read())\n",
    "                size = root.find('size')\n",
    "                \n",
    "                filename = root.find('filename').text.strip()\n",
    "                filepath = os.path.join(self.image_root, filename)\n",
    "                image_width, image_height = int(size.find('width').text), int(size.find('height').text)\n",
    "                objects = []\n",
    "                for item in root.findall('object'):\n",
    "                    object_name = item.find('name').text.strip()\n",
    "                    object_id = self.labels.index(object_name)\n",
    "                    \n",
    "                    object_bndbox = item.find('bndbox')\n",
    "                    (xmin, ymin, xmax, ymax) = [\n",
    "                        float(object_bndbox.find(key).text) for key in ['xmin', 'ymin', 'xmax', 'ymax']\n",
    "                    ]\n",
    "                    \n",
    "                    assert(object_id != -1)\n",
    "                    \n",
    "                    xmin_norm, ymin_norm = xmin / image_width, ymin / image_height\n",
    "                    width, height = xmax - xmin, ymax - ymin\n",
    "                    width_norm, height_norm = width / image_width, height / image_height\n",
    "                    xcenter_norm, ycenter_norm = xmin_norm + width_norm / 2, ymin_norm + height_norm / 2\n",
    "                    \n",
    "                    # dynamic range tricks\n",
    "                    # changes dynamic range from 0.0 to 7.0 and do floor()\n",
    "                    # -> results 0, 1, 2, 3, 4, 5, 6!\n",
    "                    cell_idx_x = math.floor(xcenter_norm * self.model_cells)\n",
    "                    cell_idx_y = math.floor(ycenter_norm * self.model_cells)\n",
    "                    \n",
    "                    cell_pos_x_norm = (xcenter_norm - (cell_idx_x / self.model_cells))\n",
    "                    cell_pos_y_norm = (ycenter_norm - (cell_idx_y / self.model_cells))\n",
    "                    \n",
    "                    objects.append([object_id, cell_idx_x, cell_idx_y, cell_pos_x_norm, cell_pos_y_norm, width_norm, height_norm])\n",
    "                \n",
    "                annotations.append((filepath, objects))\n",
    "            \n",
    "        return annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d92c8106",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7fcbe4471c14eb7bc5b001fcc0887c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Annotation 내 Object Names 검색:   0%|          | 0/5011 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed80e126992f48c2aa896630fbb2cbfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Annotation 검색:   0%|          | 0/5011 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotation[0]: C:\\Development\\dataset\\VOCdevkit\\VOC2007\\JPEGImages\\000005.jpg\n",
      "Annotation[1]: [[8, 4, 5, 0.01557142857142857, 0.01904761904761898, 0.122, 0.3413333333333333], [8, 2, 5, 0.13228571428571434, 0.13371428571428567, 0.176, 0.288], [8, 0, 5, 0.072, 0.10971428571428565, 0.124, 0.3466666666666667], [8, 3, 4, 0.10742857142857148, 0.08590476190476193, 0.108, 0.28], [8, 4, 3, 0.017571428571428682, 0.11276190476190479, 0.07, 0.09066666666666667]]\n"
     ]
    }
   ],
   "source": [
    "annotator = VOCYOLOAnnotator(\n",
    "    annotation_root=r'C:\\Development\\dataset\\VOCdevkit\\VOC2007\\Annotations',\n",
    "    image_root=r'C:\\Development\\dataset\\VOCdevkit\\VOC2007\\JPEGImages'\n",
    ")\n",
    "\n",
    "annotations = annotator.parse_annotation()\n",
    "print(\"Annotation[0]:\", annotations[0][0])\n",
    "print(\"Annotation[1]:\", annotations[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c393987e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VOCYolo(torch.utils.data.Dataset):\n",
    "    def __init__(self, labels, annotations, boxes=2, grid_size=7, transform=None):\n",
    "        super(VOCYolo, self).__init__()\n",
    "        self.labels = labels\n",
    "        self.annotations = annotations\n",
    "        self.transform = transform\n",
    "        self.boxes = boxes\n",
    "        self.grid_size = grid_size\n",
    "        self.classes = 20  # fixed as it's VOC dataset!\n",
    "        self.one_hot = torch.eye(self.classes)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        filepath, annotation = self.annotations[idx]\n",
    "        image = Image.open(filepath).convert('RGB')  # case if image is grayscale\n",
    "        label = torch.zeros((5 + self.classes, self.grid_size, self.grid_size), dtype=torch.float)\n",
    "        \n",
    "        # fill label\n",
    "        for (class_id, cell_idx_x, cell_idx_y, cell_pos_x, cell_pos_y, width, height) in annotation:\n",
    "            if label[4, cell_idx_y, cell_idx_x] != 1.0:\n",
    "                label[:5, cell_idx_y, cell_idx_x] = torch.from_numpy(np.array([cell_pos_x, cell_pos_y, width, height, 1.0], dtype=np.double))\n",
    "                label[5:, cell_idx_y, cell_idx_x] = self.one_hot[class_id]\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6758663",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YoloLoss(Module):\n",
    "    def __init__(self, lambda_coord, lambda_noobj):\n",
    "        super().__init__()\n",
    "        self.lambda_coord = lambda_coord\n",
    "        self.lambda_noobj = lambda_noobj\n",
    "        self.eps = 1e-15 # for sqrt(0), we will do sqrt(0 + eps)\n",
    "        \n",
    "    def forward(self, input: Tensor, target: Tensor) -> Tensor:\n",
    "        assert((input.shape[1] - 20) % 5 == 0)\n",
    "        bboxes = (input.shape[1] - 20) // 5\n",
    "        \n",
    "        # applies outer-model sigmoid function\n",
    "        input = torch.sigmoid(input)\n",
    "        \n",
    "        # target: [cell_pos_x, cell_pos_y, width, height, 1.0] * 7 * 7\n",
    "        object_gt_exist_mask = target[:, 4:5, :, :] == 1\n",
    "        responsible_bbox_index_mask = YoloLoss.get_responsible_bbox_predictor_mask(input, target, bboxes)\n",
    "        \n",
    "        loss = 0\n",
    "        for bbox_id in range(bboxes):\n",
    "            current_box = input[:, 5 * bbox_id:5 * (bbox_id + 1), :, :]\n",
    "            \n",
    "            # we should use with object_gt_exist_mask\n",
    "            # because non-object-existant mask will have zero index\n",
    "            current_box_responsible_mask = responsible_bbox_index_mask == bbox_id\n",
    "            \n",
    "            #! TODO: iterate over xy_loss and find out that object_gt_exist_mask works well\n",
    "            xy_loss = torch.square(current_box[:, :2, :, :] - target[:, :2, :, :]) * object_gt_exist_mask * current_box_responsible_mask\n",
    "            loss += self.lambda_coord * torch.sum(xy_loss)\n",
    "            # print(\"xy_loss \", torch.sum(xy_loss))\n",
    "            \n",
    "            wh_loss = torch.square(torch.sqrt(current_box[:, 2:4, :, :] + self.eps) - torch.sqrt(target[:, 2:4, :, :] + self.eps)) * object_gt_exist_mask * current_box_responsible_mask\n",
    "            loss += self.lambda_coord * torch.sum(wh_loss)\n",
    "            # print(\"wh_loss: \", torch.sum(wh_loss))\n",
    "            # print(\"wh_loss current_box sqrt\", torch.sum(torch.sqrt(current_box[:, 2:4, :, :] + self.eps)))\n",
    "            # print(\"wh_loss target sqrt\", torch.sum(torch.sqrt(target[:, 2:4, :, :] + self.eps)))\n",
    "            \n",
    "            conf_obj_loss = torch.square(current_box[: 4:5, :, :] - target[:, 4:5, :, :]) * object_gt_exist_mask * current_box_responsible_mask\n",
    "            conf_noobj_loss = torch.square(current_box[: 4:5, :, :] - target[:, 4:5, :, :]) * ~(object_gt_exist_mask * current_box_responsible_mask)\n",
    "            loss += torch.sum(conf_obj_loss)\n",
    "            loss += self.lambda_noobj * torch.sum(conf_noobj_loss)\n",
    "            # print(\"conf_obj_loss: \", torch.sum(conf_obj_loss))\n",
    "            # print(\"conf_noobj_loss: \", torch.sum(conf_noobj_loss))\n",
    "            \n",
    "        class_loss = torch.square(input[:, (5 * bboxes):, :, :] - target[:, 5:, :, :])\n",
    "        loss += torch.sum(class_loss)\n",
    "        # print(\"class_loss: \", torch.sum(class_loss))\n",
    "            \n",
    "        return loss\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_responsible_bbox_predictor_mask(input: Tensor, target: Tensor, bboxes: int) -> Tensor:\n",
    "        ious = []\n",
    "        for bbox_id in range(bboxes):\n",
    "            current_box_xywh = input[:, 5 * bbox_id:5 * (bbox_id + 1) - 1, :, :]\n",
    "            label_xywh = target[:, :4, :, :]\n",
    "            \n",
    "            # iou -> (B * 7 * 7)\n",
    "            iou = YoloLoss.get_iou_xywh(current_box_xywh, label_xywh)\n",
    "            ious.append(iou)\n",
    "        \n",
    "        # stacked_iou -> (B * 2 * 7 * 7)\n",
    "        stacked_iou = torch.stack(ious, dim=1)\n",
    "        # print(stacked_iou.shape, torch.argmax(stacked_iou, dim=1, keepdim=True).shape)\n",
    "        \n",
    "        return torch.argmax(stacked_iou, dim=1, keepdim=True)\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_iou_xywh(input_xywh: Tensor, label_xywh: Tensor) -> Tensor:\n",
    "        # index_map -> [1, 2, 7, 7]\n",
    "        index_map_x = torch.arange(0, 7).repeat(7)\n",
    "        index_map_y = torch.repeat_interleave(torch.arange(0, 7), 7)\n",
    "        index_map = torch.unsqueeze(torch.stack([index_map_y, index_map_x], dim=0).view(2, 7, 7), 0)\n",
    "        \n",
    "        if input_xywh.device.type == 'cuda':\n",
    "            index_map = index_map.cuda(non_blocking=True)\n",
    "        \n",
    "        input_xy_global = (input_xywh[:, :2, :, :] + index_map) / 7\n",
    "        input_width_half, input_height_half = (input_xywh[:, 2, :, :] / 2), (input_xywh[:, 3, :, :] / 2)\n",
    "        input_xmin = input_xy_global[:, 0, :, :] - input_width_half  # x_center - width / 2\n",
    "        input_xmax = input_xy_global[:, 0, :, :] + input_width_half\n",
    "        input_ymin = input_xy_global[:, 1, :, :] - input_height_half\n",
    "        input_ymax = input_xy_global[:, 1, :, :] + input_height_half\n",
    "        \n",
    "        label_xy_global = (label_xywh[:, :2, :, :] + index_map) / 7\n",
    "        label_width_half, label_height_half = (label_xywh[:, 2, :, :] / 2), (label_xywh[:, 3, :, :] / 2)\n",
    "        label_xmin = label_xy_global[:, 0, :, :] - label_width_half  # x_center - width / 2\n",
    "        label_xmax = label_xy_global[:, 0, :, :] + label_width_half\n",
    "        label_ymin = label_xy_global[:, 1, :, :] - label_height_half\n",
    "        label_ymax = label_xy_global[:, 1, :, :] + label_height_half\n",
    "\n",
    "        input_volume = input_xywh[:, 2, :, :] * input_xywh[:, 3, :, :]\n",
    "        label_volume = label_xywh[:, 2, :, :] * label_xywh[:, 3, :, :]\n",
    "        intersect_width = torch.minimum(input_xmax, label_xmax) - torch.maximum(input_xmin, label_xmin)\n",
    "        intersect_height = torch.minimum(input_ymax, label_ymax) - torch.maximum(input_ymin, label_ymin)\n",
    "        intersect_volume = intersect_width * intersect_height\n",
    "        union_volume = input_volume + label_volume - intersect_volume\n",
    "        \n",
    "        return intersect_volume / union_volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1642cdb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0/2506: Loss 618.048279\n",
      "Batch 1/2506: Loss 499.237732\n",
      "Batch 2/2506: Loss 56.602154\n",
      "Batch 3/2506: Loss 57.019966\n",
      "Batch 4/2506: Loss 85.452438\n",
      "Batch 5/2506: Loss 66.465179\n",
      "Batch 6/2506: Loss 78.577988\n",
      "Batch 7/2506: Loss 50.668087\n",
      "Batch 8/2506: Loss 116.307594\n",
      "Batch 9/2506: Loss 58.737648\n",
      "Batch 10/2506: Loss 55.312134\n",
      "Batch 11/2506: Loss 127.595505\n",
      "Batch 12/2506: Loss 33.996422\n",
      "Batch 13/2506: Loss 43.574802\n",
      "Batch 14/2506: Loss 71.114807\n",
      "Batch 15/2506: Loss 152.329086\n",
      "Batch 16/2506: Loss 46.574753\n",
      "Batch 17/2506: Loss 76.560638\n",
      "Batch 18/2506: Loss 108.858917\n",
      "Batch 19/2506: Loss 52.793404\n",
      "Batch 20/2506: Loss 49.422722\n",
      "Batch 21/2506: Loss 128.460236\n",
      "Batch 22/2506: Loss 35.748432\n",
      "Batch 23/2506: Loss 112.799667\n",
      "Batch 24/2506: Loss 71.529457\n",
      "Batch 25/2506: Loss 50.057350\n",
      "Batch 26/2506: Loss 79.207367\n",
      "Batch 27/2506: Loss 54.300194\n",
      "Batch 28/2506: Loss 47.407223\n",
      "Batch 29/2506: Loss 78.266174\n",
      "Batch 30/2506: Loss 55.148899\n",
      "Batch 31/2506: Loss 142.754776\n",
      "Batch 32/2506: Loss 91.361008\n",
      "Batch 33/2506: Loss 77.399956\n",
      "Batch 34/2506: Loss 79.990829\n",
      "Batch 35/2506: Loss 80.923157\n",
      "Batch 36/2506: Loss 46.812702\n",
      "Batch 37/2506: Loss 132.711243\n",
      "Batch 38/2506: Loss 61.809540\n",
      "Batch 39/2506: Loss 48.048649\n",
      "Batch 40/2506: Loss 95.855743\n",
      "Batch 41/2506: Loss 69.908440\n",
      "Batch 42/2506: Loss 39.501045\n",
      "Batch 43/2506: Loss 40.858482\n",
      "Batch 44/2506: Loss 113.310638\n",
      "Batch 45/2506: Loss 68.562744\n",
      "Batch 46/2506: Loss 52.578159\n",
      "Batch 47/2506: Loss 40.966240\n",
      "Batch 48/2506: Loss 117.932114\n",
      "Batch 49/2506: Loss 61.215042\n",
      "Batch 50/2506: Loss 102.745865\n",
      "Batch 51/2506: Loss 149.817703\n",
      "Batch 52/2506: Loss 53.274975\n",
      "Batch 53/2506: Loss 61.195503\n",
      "Batch 54/2506: Loss 39.390472\n",
      "Batch 55/2506: Loss 74.330414\n",
      "Batch 56/2506: Loss 44.113396\n",
      "Batch 57/2506: Loss 85.362015\n",
      "Batch 58/2506: Loss 65.057709\n",
      "Batch 59/2506: Loss 66.708801\n",
      "Batch 60/2506: Loss 31.053106\n",
      "Batch 61/2506: Loss 94.498871\n",
      "Batch 62/2506: Loss 64.233971\n",
      "Batch 63/2506: Loss 54.590443\n",
      "Batch 64/2506: Loss 189.385559\n",
      "Batch 65/2506: Loss 145.256775\n",
      "Batch 66/2506: Loss 68.857529\n",
      "Batch 67/2506: Loss 80.482681\n",
      "Batch 68/2506: Loss 70.558121\n",
      "Batch 69/2506: Loss 66.835632\n",
      "Batch 70/2506: Loss 103.603043\n",
      "Batch 71/2506: Loss 90.325089\n",
      "Batch 72/2506: Loss 57.727001\n",
      "Batch 73/2506: Loss 76.940002\n",
      "Batch 74/2506: Loss 46.610321\n",
      "Batch 75/2506: Loss 99.503365\n",
      "Batch 76/2506: Loss 88.441879\n",
      "Batch 77/2506: Loss 45.109337\n",
      "Batch 78/2506: Loss 42.660267\n",
      "Batch 79/2506: Loss 50.191833\n",
      "Batch 80/2506: Loss 94.457031\n",
      "Batch 81/2506: Loss 93.320320\n",
      "Batch 82/2506: Loss 57.280594\n",
      "Batch 83/2506: Loss 114.799362\n",
      "Batch 84/2506: Loss 56.058395\n",
      "Batch 85/2506: Loss 52.346069\n",
      "Batch 86/2506: Loss 47.388508\n",
      "Batch 87/2506: Loss 40.088142\n",
      "Batch 88/2506: Loss 53.623985\n",
      "Batch 89/2506: Loss 99.790497\n",
      "Batch 90/2506: Loss 54.775551\n",
      "Batch 91/2506: Loss 88.936844\n",
      "Batch 92/2506: Loss 58.345131\n",
      "Batch 93/2506: Loss 91.714775\n",
      "Batch 94/2506: Loss 56.549072\n",
      "Batch 95/2506: Loss 126.933502\n",
      "Batch 96/2506: Loss 102.356308\n",
      "Batch 97/2506: Loss 108.749863\n",
      "Batch 98/2506: Loss 44.179943\n",
      "Batch 99/2506: Loss 56.202415\n",
      "Batch 100/2506: Loss 41.139400\n",
      "Batch 101/2506: Loss 190.534332\n",
      "Batch 102/2506: Loss 87.983368\n",
      "Batch 103/2506: Loss 98.946846\n",
      "Batch 104/2506: Loss 149.003632\n",
      "Batch 105/2506: Loss 51.692848\n",
      "Batch 106/2506: Loss 31.886024\n",
      "Batch 107/2506: Loss 131.496017\n",
      "Batch 108/2506: Loss 40.354713\n",
      "Batch 109/2506: Loss 40.563885\n",
      "Batch 110/2506: Loss 31.735275\n",
      "Batch 111/2506: Loss 110.930664\n",
      "Batch 112/2506: Loss 115.490509\n",
      "Batch 113/2506: Loss 93.787415\n",
      "Batch 114/2506: Loss 81.911774\n",
      "Batch 115/2506: Loss 101.853386\n",
      "Batch 116/2506: Loss 42.832184\n",
      "Batch 117/2506: Loss 99.546249\n",
      "Batch 118/2506: Loss 88.751320\n",
      "Batch 119/2506: Loss 50.247810\n",
      "Batch 120/2506: Loss 131.859161\n",
      "Batch 121/2506: Loss 75.494675\n",
      "Batch 122/2506: Loss 63.677803\n",
      "Batch 123/2506: Loss 164.920502\n",
      "Batch 124/2506: Loss 51.101410\n",
      "Batch 125/2506: Loss 59.343540\n",
      "Batch 126/2506: Loss 62.533997\n",
      "Batch 127/2506: Loss 86.914307\n",
      "Batch 128/2506: Loss 39.377174\n",
      "Batch 129/2506: Loss 55.021431\n",
      "Batch 130/2506: Loss 61.491089\n",
      "Batch 131/2506: Loss 40.840836\n",
      "Batch 132/2506: Loss 182.572052\n",
      "Batch 133/2506: Loss 118.776535\n",
      "Batch 134/2506: Loss 61.972515\n",
      "Batch 135/2506: Loss 71.320450\n",
      "Batch 136/2506: Loss 60.743538\n",
      "Batch 137/2506: Loss 67.621384\n",
      "Batch 138/2506: Loss 45.088646\n",
      "Batch 139/2506: Loss 84.637215\n",
      "Batch 140/2506: Loss 92.908661\n",
      "Batch 141/2506: Loss 37.706348\n",
      "Batch 142/2506: Loss 42.479660\n",
      "Batch 143/2506: Loss 89.880699\n",
      "Batch 144/2506: Loss 127.441727\n",
      "Batch 145/2506: Loss 58.833405\n",
      "Batch 146/2506: Loss 81.592659\n",
      "Batch 147/2506: Loss 72.765259\n",
      "Batch 148/2506: Loss 69.818359\n",
      "Batch 149/2506: Loss 52.291660\n",
      "Batch 150/2506: Loss 74.897720\n",
      "Batch 151/2506: Loss 56.362854\n",
      "Batch 152/2506: Loss 167.505188\n",
      "Batch 153/2506: Loss 64.710777\n",
      "Batch 154/2506: Loss 46.183662\n",
      "Batch 155/2506: Loss 144.283173\n",
      "Batch 156/2506: Loss 51.768524\n",
      "Batch 157/2506: Loss 58.665436\n",
      "Batch 158/2506: Loss 86.722260\n",
      "Batch 159/2506: Loss 100.211319\n",
      "Batch 160/2506: Loss 68.202316\n",
      "Batch 161/2506: Loss 39.549763\n",
      "Batch 162/2506: Loss 73.600983\n",
      "Batch 163/2506: Loss 201.444321\n",
      "Batch 164/2506: Loss 35.991009\n",
      "Batch 165/2506: Loss 53.375103\n",
      "Batch 166/2506: Loss 71.963066\n",
      "Batch 167/2506: Loss 113.139282\n",
      "Batch 168/2506: Loss 100.736755\n",
      "Batch 169/2506: Loss 105.971169\n",
      "Batch 170/2506: Loss 64.187767\n",
      "Batch 171/2506: Loss 94.106659\n",
      "Batch 172/2506: Loss 46.829472\n",
      "Batch 173/2506: Loss 110.402756\n",
      "Batch 174/2506: Loss 30.435282\n",
      "Batch 175/2506: Loss 49.183445\n",
      "Batch 176/2506: Loss 42.430130\n",
      "Batch 177/2506: Loss 91.729713\n",
      "Batch 178/2506: Loss 47.713379\n",
      "Batch 179/2506: Loss 144.124542\n",
      "Batch 180/2506: Loss 130.871887\n",
      "Batch 181/2506: Loss 66.333786\n",
      "Batch 182/2506: Loss 113.367920\n",
      "Batch 183/2506: Loss 75.544815\n",
      "Batch 184/2506: Loss 99.145622\n",
      "Batch 185/2506: Loss 62.013649\n",
      "Batch 186/2506: Loss 44.263741\n",
      "Batch 187/2506: Loss 78.138580\n",
      "Batch 188/2506: Loss 57.387875\n",
      "Batch 189/2506: Loss 122.287445\n",
      "Batch 190/2506: Loss 127.303970\n",
      "Batch 191/2506: Loss 64.405769\n",
      "Batch 192/2506: Loss 126.222321\n",
      "Batch 193/2506: Loss 182.240875\n",
      "Batch 194/2506: Loss 58.714817\n",
      "Batch 195/2506: Loss 61.156059\n",
      "Batch 196/2506: Loss 97.965714\n",
      "Batch 197/2506: Loss 83.585960\n",
      "Batch 198/2506: Loss 60.711811\n",
      "Batch 199/2506: Loss 157.786972\n",
      "Batch 200/2506: Loss 33.073986\n",
      "Batch 201/2506: Loss 73.340195\n",
      "Batch 202/2506: Loss 55.473549\n",
      "Batch 203/2506: Loss 158.442703\n",
      "Batch 204/2506: Loss 26.715897\n",
      "Batch 205/2506: Loss 83.275543\n",
      "Batch 206/2506: Loss 45.204643\n",
      "Batch 207/2506: Loss 33.150303\n",
      "Batch 208/2506: Loss 52.057518\n",
      "Batch 209/2506: Loss 50.369019\n",
      "Batch 210/2506: Loss 59.937916\n",
      "Batch 211/2506: Loss 80.742020\n",
      "Batch 212/2506: Loss 28.193710\n",
      "Batch 213/2506: Loss 101.455391\n",
      "Batch 214/2506: Loss 45.456730\n",
      "Batch 215/2506: Loss 41.391884\n",
      "Batch 216/2506: Loss 76.460297\n",
      "Batch 217/2506: Loss 27.586334\n",
      "Batch 218/2506: Loss 89.713654\n",
      "Batch 219/2506: Loss 70.290077\n",
      "Batch 220/2506: Loss 69.455643\n",
      "Batch 221/2506: Loss 137.803894\n",
      "Batch 222/2506: Loss 144.655441\n",
      "Batch 223/2506: Loss 65.333694\n",
      "Batch 224/2506: Loss 42.961029\n",
      "Batch 225/2506: Loss 37.933815\n",
      "Batch 226/2506: Loss 57.148445\n",
      "Batch 227/2506: Loss 132.704544\n",
      "Batch 228/2506: Loss 30.418003\n",
      "Batch 229/2506: Loss 47.793617\n",
      "Batch 230/2506: Loss 56.699829\n",
      "Batch 231/2506: Loss 62.010265\n",
      "Batch 232/2506: Loss 104.543427\n",
      "Batch 233/2506: Loss 89.992676\n",
      "Batch 234/2506: Loss 154.766785\n",
      "Batch 235/2506: Loss 68.525986\n",
      "Batch 236/2506: Loss 59.636894\n",
      "Batch 237/2506: Loss 53.790550\n",
      "Batch 238/2506: Loss 50.269684\n",
      "Batch 239/2506: Loss 55.061539\n",
      "Batch 240/2506: Loss 47.252235\n",
      "Batch 241/2506: Loss 91.248466\n",
      "Batch 242/2506: Loss 33.792763\n",
      "Batch 243/2506: Loss 101.884010\n",
      "Batch 244/2506: Loss 125.423012\n",
      "Batch 245/2506: Loss 71.948532\n",
      "Batch 246/2506: Loss 117.414879\n",
      "Batch 247/2506: Loss 89.294357\n",
      "Batch 248/2506: Loss 59.983013\n",
      "Batch 249/2506: Loss 119.066757\n",
      "Batch 250/2506: Loss 108.034424\n",
      "Batch 251/2506: Loss 89.405594\n",
      "Batch 252/2506: Loss 72.940453\n",
      "Batch 253/2506: Loss 118.222443\n",
      "Batch 254/2506: Loss 49.972580\n",
      "Batch 255/2506: Loss 72.850800\n",
      "Batch 256/2506: Loss 64.817230\n",
      "Batch 257/2506: Loss 43.249634\n",
      "Batch 258/2506: Loss 93.640457\n",
      "Batch 259/2506: Loss 43.044907\n",
      "Batch 260/2506: Loss 55.040932\n",
      "Batch 261/2506: Loss 41.644241\n",
      "Batch 262/2506: Loss 55.041672\n",
      "Batch 263/2506: Loss 100.081184\n",
      "Batch 264/2506: Loss 53.612301\n",
      "Batch 265/2506: Loss 101.698128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 266/2506: Loss 74.054733\n",
      "Batch 267/2506: Loss 79.454926\n",
      "Batch 268/2506: Loss 161.538147\n",
      "Batch 269/2506: Loss 109.062759\n",
      "Batch 270/2506: Loss 69.401093\n",
      "Batch 271/2506: Loss 72.583176\n",
      "Batch 272/2506: Loss 56.388828\n",
      "Batch 273/2506: Loss 47.489166\n",
      "Batch 274/2506: Loss 37.712456\n",
      "Batch 275/2506: Loss 111.603836\n",
      "Batch 276/2506: Loss 91.053268\n",
      "Batch 277/2506: Loss 121.248108\n",
      "Batch 278/2506: Loss 59.180359\n",
      "Batch 279/2506: Loss 61.974438\n",
      "Batch 280/2506: Loss 54.340183\n",
      "Batch 281/2506: Loss 58.085903\n",
      "Batch 282/2506: Loss 94.010544\n",
      "Batch 283/2506: Loss 50.452454\n",
      "Batch 284/2506: Loss 78.115417\n",
      "Batch 285/2506: Loss 78.600258\n",
      "Batch 286/2506: Loss 49.458721\n",
      "Batch 287/2506: Loss 30.897366\n",
      "Batch 288/2506: Loss 53.187279\n",
      "Batch 289/2506: Loss 107.692429\n",
      "Batch 290/2506: Loss 65.283562\n",
      "Batch 291/2506: Loss 81.658882\n",
      "Batch 292/2506: Loss 50.448631\n",
      "Batch 293/2506: Loss 62.013943\n",
      "Batch 294/2506: Loss 34.828739\n",
      "Batch 295/2506: Loss 43.874695\n",
      "Batch 296/2506: Loss 69.278831\n",
      "Batch 297/2506: Loss 55.563477\n",
      "Batch 298/2506: Loss 83.118881\n",
      "Batch 299/2506: Loss 81.942749\n",
      "Batch 300/2506: Loss 47.825096\n",
      "Batch 301/2506: Loss 69.882957\n",
      "Batch 302/2506: Loss 36.631683\n",
      "Batch 303/2506: Loss 80.496033\n",
      "Batch 304/2506: Loss 47.945904\n",
      "Batch 305/2506: Loss 115.649933\n",
      "Batch 306/2506: Loss 88.447517\n",
      "Batch 307/2506: Loss 73.496582\n",
      "Batch 308/2506: Loss 83.547287\n",
      "Batch 309/2506: Loss 26.930511\n",
      "Batch 310/2506: Loss 71.877831\n",
      "Batch 311/2506: Loss 107.621765\n",
      "Batch 312/2506: Loss 70.301086\n",
      "Batch 313/2506: Loss 74.410217\n",
      "Batch 314/2506: Loss 70.498383\n",
      "Batch 315/2506: Loss 59.697723\n",
      "Batch 316/2506: Loss 50.866379\n",
      "Batch 317/2506: Loss 61.631058\n",
      "Batch 318/2506: Loss 96.703613\n",
      "Batch 319/2506: Loss 63.404037\n",
      "Batch 320/2506: Loss 44.136845\n",
      "Batch 321/2506: Loss 60.829376\n",
      "Batch 322/2506: Loss 105.691437\n",
      "Batch 323/2506: Loss 51.388580\n",
      "Batch 324/2506: Loss 27.256184\n",
      "Batch 325/2506: Loss 67.803047\n",
      "Batch 326/2506: Loss 58.443577\n",
      "Batch 327/2506: Loss 74.236885\n",
      "Batch 328/2506: Loss 83.239983\n",
      "Batch 329/2506: Loss 70.678085\n",
      "Batch 330/2506: Loss 59.014687\n",
      "Batch 331/2506: Loss 78.101730\n",
      "Batch 332/2506: Loss 56.246887\n",
      "Batch 333/2506: Loss 58.003639\n",
      "Batch 334/2506: Loss 170.776505\n",
      "Batch 335/2506: Loss 107.308823\n",
      "Batch 336/2506: Loss 153.446808\n",
      "Batch 337/2506: Loss 37.360313\n",
      "Batch 338/2506: Loss 80.485764\n",
      "Batch 339/2506: Loss 91.422310\n",
      "Batch 340/2506: Loss 205.912598\n",
      "Batch 341/2506: Loss 74.831566\n",
      "Batch 342/2506: Loss 55.306698\n",
      "Batch 343/2506: Loss 131.014633\n",
      "Batch 344/2506: Loss 96.891670\n",
      "Batch 345/2506: Loss 72.742188\n",
      "Batch 346/2506: Loss 108.967705\n",
      "Batch 347/2506: Loss 122.562553\n",
      "Batch 348/2506: Loss 62.232681\n",
      "Batch 349/2506: Loss 52.913452\n",
      "Batch 350/2506: Loss 58.847977\n",
      "Batch 351/2506: Loss 43.760235\n",
      "Batch 352/2506: Loss 55.595760\n",
      "Batch 353/2506: Loss 84.615356\n",
      "Batch 354/2506: Loss 225.101318\n",
      "Batch 355/2506: Loss 51.751503\n",
      "Batch 356/2506: Loss 79.939713\n",
      "Batch 357/2506: Loss 88.518456\n",
      "Batch 358/2506: Loss 86.509064\n",
      "Batch 359/2506: Loss 30.523312\n",
      "Batch 360/2506: Loss 42.561123\n",
      "Batch 361/2506: Loss 55.661057\n",
      "Batch 362/2506: Loss 129.482544\n",
      "Batch 363/2506: Loss 68.483238\n",
      "Batch 364/2506: Loss 94.747704\n",
      "Batch 365/2506: Loss 92.850922\n",
      "Batch 366/2506: Loss 140.711761\n",
      "Batch 367/2506: Loss 64.723000\n",
      "Batch 368/2506: Loss 88.677002\n",
      "Batch 369/2506: Loss 91.773918\n",
      "Batch 370/2506: Loss 53.831436\n",
      "Batch 371/2506: Loss 63.519711\n",
      "Batch 372/2506: Loss 146.411057\n",
      "Batch 373/2506: Loss 77.753731\n",
      "Batch 374/2506: Loss 60.893562\n",
      "Batch 375/2506: Loss 42.577629\n",
      "Batch 376/2506: Loss 92.157967\n",
      "Batch 377/2506: Loss 42.412350\n",
      "Batch 378/2506: Loss 122.148666\n",
      "Batch 379/2506: Loss 36.304470\n",
      "Batch 380/2506: Loss 107.764496\n",
      "Batch 381/2506: Loss 71.589371\n",
      "Batch 382/2506: Loss 30.316313\n",
      "Batch 383/2506: Loss 170.987030\n",
      "Batch 384/2506: Loss 111.363976\n",
      "Batch 385/2506: Loss 47.257881\n",
      "Batch 386/2506: Loss 109.812637\n",
      "Batch 387/2506: Loss 126.947472\n",
      "Batch 388/2506: Loss 53.446213\n",
      "Batch 389/2506: Loss 61.170040\n",
      "Batch 390/2506: Loss 86.863930\n",
      "Batch 391/2506: Loss 41.372185\n",
      "Batch 392/2506: Loss 72.651443\n",
      "Batch 393/2506: Loss 90.627090\n",
      "Batch 394/2506: Loss 155.338745\n",
      "Batch 395/2506: Loss 61.182617\n",
      "Batch 396/2506: Loss 90.908127\n",
      "Batch 397/2506: Loss 89.107918\n",
      "Batch 398/2506: Loss 53.667957\n",
      "Batch 399/2506: Loss 31.359863\n",
      "Batch 400/2506: Loss 86.288223\n",
      "Batch 401/2506: Loss 141.214874\n",
      "Batch 402/2506: Loss 43.870712\n",
      "Batch 403/2506: Loss 81.095505\n",
      "Batch 404/2506: Loss 116.189667\n",
      "Batch 405/2506: Loss 77.234695\n",
      "Batch 406/2506: Loss 198.457184\n",
      "Batch 407/2506: Loss 141.905884\n",
      "Batch 408/2506: Loss 106.125061\n",
      "Batch 409/2506: Loss 65.546951\n",
      "Batch 410/2506: Loss 95.612152\n",
      "Batch 411/2506: Loss 44.239098\n",
      "Batch 412/2506: Loss 137.554993\n",
      "Batch 413/2506: Loss 45.678692\n",
      "Batch 414/2506: Loss 134.024292\n",
      "Batch 415/2506: Loss 125.622269\n",
      "Batch 416/2506: Loss 163.546265\n",
      "Batch 417/2506: Loss 61.873627\n",
      "Batch 418/2506: Loss 110.208107\n",
      "Batch 419/2506: Loss 81.866226\n",
      "Batch 420/2506: Loss 38.930508\n",
      "Batch 421/2506: Loss 82.383842\n",
      "Batch 422/2506: Loss 104.328232\n",
      "Batch 423/2506: Loss 79.095703\n",
      "Batch 424/2506: Loss 55.799305\n",
      "Batch 425/2506: Loss 93.954666\n",
      "Batch 426/2506: Loss 71.777008\n",
      "Batch 427/2506: Loss 39.570618\n",
      "Batch 428/2506: Loss 98.467056\n",
      "Batch 429/2506: Loss 73.680809\n",
      "Batch 430/2506: Loss 62.257896\n",
      "Batch 431/2506: Loss 75.736519\n",
      "Batch 432/2506: Loss 85.557884\n",
      "Batch 433/2506: Loss 65.100998\n",
      "Batch 434/2506: Loss 78.498802\n",
      "Batch 435/2506: Loss 86.251740\n",
      "Batch 436/2506: Loss 49.413898\n",
      "Batch 437/2506: Loss 123.113541\n",
      "Batch 438/2506: Loss 113.939911\n",
      "Batch 439/2506: Loss 52.624249\n",
      "Batch 440/2506: Loss 87.925949\n",
      "Batch 441/2506: Loss 54.927677\n",
      "Batch 442/2506: Loss 173.880127\n",
      "Batch 443/2506: Loss 48.321007\n",
      "Batch 444/2506: Loss 41.987488\n",
      "Batch 445/2506: Loss 68.739166\n",
      "Batch 446/2506: Loss 116.804436\n",
      "Batch 447/2506: Loss 121.925644\n",
      "Batch 448/2506: Loss 53.754601\n",
      "Batch 449/2506: Loss 97.439163\n",
      "Batch 450/2506: Loss 77.620041\n",
      "Batch 451/2506: Loss 49.151012\n",
      "Batch 452/2506: Loss 38.363041\n",
      "Batch 453/2506: Loss 39.883041\n",
      "Batch 454/2506: Loss 72.607086\n",
      "Batch 455/2506: Loss 110.475754\n",
      "Batch 456/2506: Loss 54.408203\n",
      "Batch 457/2506: Loss 51.339417\n",
      "Batch 458/2506: Loss 131.122070\n",
      "Batch 459/2506: Loss 39.874619\n",
      "Batch 460/2506: Loss 37.680492\n",
      "Batch 461/2506: Loss 89.988274\n",
      "Batch 462/2506: Loss 72.368187\n",
      "Batch 463/2506: Loss 115.502975\n",
      "Batch 464/2506: Loss 101.467384\n",
      "Batch 465/2506: Loss 50.671104\n",
      "Batch 466/2506: Loss 38.101631\n",
      "Batch 467/2506: Loss 47.864204\n",
      "Batch 468/2506: Loss 69.635521\n",
      "Batch 469/2506: Loss 116.258011\n",
      "Batch 470/2506: Loss 95.274071\n",
      "Batch 471/2506: Loss 113.096519\n",
      "Batch 472/2506: Loss 98.160187\n",
      "Batch 473/2506: Loss 99.315979\n",
      "Batch 474/2506: Loss 157.560211\n",
      "Batch 475/2506: Loss 40.192516\n",
      "Batch 476/2506: Loss 120.411552\n",
      "Batch 477/2506: Loss 94.536285\n",
      "Batch 478/2506: Loss 99.749924\n",
      "Batch 479/2506: Loss 52.535309\n",
      "Batch 480/2506: Loss 66.354042\n",
      "Batch 481/2506: Loss 58.153091\n",
      "Batch 482/2506: Loss 75.499924\n",
      "Batch 483/2506: Loss 108.889633\n",
      "Batch 484/2506: Loss 45.072556\n",
      "Batch 485/2506: Loss 108.233978\n",
      "Batch 486/2506: Loss 64.865967\n",
      "Batch 487/2506: Loss 59.884380\n",
      "Batch 488/2506: Loss 68.091171\n",
      "Batch 489/2506: Loss 32.133888\n",
      "Batch 490/2506: Loss 37.965637\n",
      "Batch 491/2506: Loss 65.157349\n",
      "Batch 492/2506: Loss 107.324341\n",
      "Batch 493/2506: Loss 42.965851\n",
      "Batch 494/2506: Loss 45.066925\n",
      "Batch 495/2506: Loss 42.752563\n",
      "Batch 496/2506: Loss 43.352818\n",
      "Batch 497/2506: Loss 60.823326\n",
      "Batch 498/2506: Loss 106.264877\n",
      "Batch 499/2506: Loss 81.455765\n",
      "Batch 500/2506: Loss 117.868591\n",
      "Batch 501/2506: Loss 51.217937\n",
      "Batch 502/2506: Loss 71.481064\n",
      "Batch 503/2506: Loss 81.523972\n",
      "Batch 504/2506: Loss 56.039547\n",
      "Batch 505/2506: Loss 79.974472\n",
      "Batch 506/2506: Loss 44.558495\n",
      "Batch 507/2506: Loss 78.632317\n",
      "Batch 508/2506: Loss 71.125763\n",
      "Batch 509/2506: Loss 163.829163\n",
      "Batch 510/2506: Loss 159.094681\n",
      "Batch 511/2506: Loss 100.304604\n",
      "Batch 512/2506: Loss 41.145508\n",
      "Batch 513/2506: Loss 46.661560\n",
      "Batch 514/2506: Loss 76.304001\n",
      "Batch 515/2506: Loss 33.693710\n",
      "Batch 516/2506: Loss 40.096737\n",
      "Batch 517/2506: Loss 111.026932\n",
      "Batch 518/2506: Loss 55.975426\n",
      "Batch 519/2506: Loss 54.681847\n",
      "Batch 520/2506: Loss 43.362350\n",
      "Batch 521/2506: Loss 74.129654\n",
      "Batch 522/2506: Loss 66.074463\n",
      "Batch 523/2506: Loss 30.549511\n",
      "Batch 524/2506: Loss 123.215797\n",
      "Batch 525/2506: Loss 44.118958\n",
      "Batch 526/2506: Loss 71.259392\n",
      "Batch 527/2506: Loss 115.850220\n",
      "Batch 528/2506: Loss 114.825897\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 529/2506: Loss 67.684402\n",
      "Batch 530/2506: Loss 36.767952\n",
      "Batch 531/2506: Loss 44.975563\n",
      "Batch 532/2506: Loss 147.072876\n",
      "Batch 533/2506: Loss 36.673431\n",
      "Batch 534/2506: Loss 79.318756\n",
      "Batch 535/2506: Loss 65.797989\n",
      "Batch 536/2506: Loss 55.813705\n",
      "Batch 537/2506: Loss 39.804695\n",
      "Batch 538/2506: Loss 176.589752\n",
      "Batch 539/2506: Loss 64.966515\n",
      "Batch 540/2506: Loss 43.035301\n",
      "Batch 541/2506: Loss 70.513016\n",
      "Batch 542/2506: Loss 75.990189\n",
      "Batch 543/2506: Loss 51.476345\n",
      "Batch 544/2506: Loss 97.375435\n",
      "Batch 545/2506: Loss 63.357605\n",
      "Batch 546/2506: Loss 66.089828\n",
      "Batch 547/2506: Loss 76.970200\n",
      "Batch 548/2506: Loss 64.447952\n",
      "Batch 549/2506: Loss 104.721893\n",
      "Batch 550/2506: Loss 195.436050\n",
      "Batch 551/2506: Loss 83.237564\n",
      "Batch 552/2506: Loss 40.387856\n",
      "Batch 553/2506: Loss 58.484230\n",
      "Batch 554/2506: Loss 44.103485\n",
      "Batch 555/2506: Loss 68.830643\n",
      "Batch 556/2506: Loss 86.156403\n",
      "Batch 557/2506: Loss 40.213058\n",
      "Batch 558/2506: Loss 144.690186\n",
      "Batch 559/2506: Loss 51.486324\n",
      "Batch 560/2506: Loss 64.238403\n",
      "Batch 561/2506: Loss 102.104889\n",
      "Batch 562/2506: Loss 44.016907\n",
      "Batch 563/2506: Loss 61.696915\n",
      "Batch 564/2506: Loss 60.154572\n",
      "Batch 565/2506: Loss 69.855659\n",
      "Batch 566/2506: Loss 85.955078\n",
      "Batch 567/2506: Loss 49.556423\n",
      "Batch 568/2506: Loss 59.748661\n",
      "Batch 569/2506: Loss 117.106880\n",
      "Batch 570/2506: Loss 65.582458\n",
      "Batch 571/2506: Loss 53.344780\n",
      "Batch 572/2506: Loss 85.727402\n",
      "Batch 573/2506: Loss 52.833435\n",
      "Batch 574/2506: Loss 40.941662\n",
      "Batch 575/2506: Loss 39.413101\n",
      "Batch 576/2506: Loss 135.703476\n",
      "Batch 577/2506: Loss 40.404968\n",
      "Batch 578/2506: Loss 58.162323\n",
      "Batch 579/2506: Loss 73.716660\n",
      "Batch 580/2506: Loss 40.196438\n",
      "Batch 581/2506: Loss 87.015953\n",
      "Batch 582/2506: Loss 57.650295\n",
      "Batch 583/2506: Loss 86.984543\n",
      "Batch 584/2506: Loss 107.044495\n",
      "Batch 585/2506: Loss 45.832550\n",
      "Batch 586/2506: Loss 48.671494\n",
      "Batch 587/2506: Loss 79.794876\n",
      "Batch 588/2506: Loss 115.370636\n",
      "Batch 589/2506: Loss 49.073227\n",
      "Batch 590/2506: Loss 56.417744\n",
      "Batch 591/2506: Loss 45.919212\n",
      "Batch 592/2506: Loss 27.309759\n",
      "Batch 593/2506: Loss 66.468140\n",
      "Batch 594/2506: Loss 106.436852\n",
      "Batch 595/2506: Loss 54.589607\n",
      "Batch 596/2506: Loss 106.923035\n",
      "Batch 597/2506: Loss 83.980110\n",
      "Batch 598/2506: Loss 84.082489\n",
      "Batch 599/2506: Loss 57.913826\n",
      "Batch 600/2506: Loss 51.818550\n",
      "Batch 601/2506: Loss 50.515965\n",
      "Batch 602/2506: Loss 104.872917\n",
      "Batch 603/2506: Loss 69.861816\n",
      "Batch 604/2506: Loss 89.562119\n",
      "Batch 605/2506: Loss 182.706635\n",
      "Batch 606/2506: Loss 53.400608\n",
      "Batch 607/2506: Loss 101.007172\n",
      "Batch 608/2506: Loss 58.322571\n",
      "Batch 609/2506: Loss 43.009827\n",
      "Batch 610/2506: Loss 88.658478\n",
      "Batch 611/2506: Loss 80.126480\n",
      "Batch 612/2506: Loss 93.300705\n",
      "Batch 613/2506: Loss 40.476513\n",
      "Batch 614/2506: Loss 109.157524\n",
      "Batch 615/2506: Loss 100.398300\n",
      "Batch 616/2506: Loss 58.250290\n",
      "Batch 617/2506: Loss 66.618256\n",
      "Batch 618/2506: Loss 128.472366\n",
      "Batch 619/2506: Loss 60.583996\n",
      "Batch 620/2506: Loss 46.849640\n",
      "Batch 621/2506: Loss 49.814026\n",
      "Batch 622/2506: Loss 75.827209\n",
      "Batch 623/2506: Loss 71.341400\n",
      "Batch 624/2506: Loss 100.459824\n",
      "Batch 625/2506: Loss 78.302467\n",
      "Batch 626/2506: Loss 37.241131\n",
      "Batch 627/2506: Loss 65.953545\n",
      "Batch 628/2506: Loss 63.611336\n",
      "Batch 629/2506: Loss 28.634396\n",
      "Batch 630/2506: Loss 53.566772\n",
      "Batch 631/2506: Loss 71.293030\n",
      "Batch 632/2506: Loss 106.666336\n",
      "Batch 633/2506: Loss 100.019012\n",
      "Batch 634/2506: Loss 222.875519\n",
      "Batch 635/2506: Loss 127.277252\n",
      "Batch 636/2506: Loss 31.601086\n",
      "Batch 637/2506: Loss 40.413792\n",
      "Batch 638/2506: Loss 125.634544\n",
      "Batch 639/2506: Loss 44.386414\n",
      "Batch 640/2506: Loss 29.272572\n",
      "Batch 641/2506: Loss 40.304764\n",
      "Batch 642/2506: Loss 137.218216\n",
      "Batch 643/2506: Loss 66.849335\n",
      "Batch 644/2506: Loss 84.108490\n",
      "Batch 645/2506: Loss 44.032635\n",
      "Batch 646/2506: Loss 46.411465\n",
      "Batch 647/2506: Loss 74.113060\n",
      "Batch 648/2506: Loss 93.426720\n",
      "Batch 649/2506: Loss 74.476181\n",
      "Batch 650/2506: Loss 206.549744\n",
      "Batch 651/2506: Loss 41.466530\n",
      "Batch 652/2506: Loss 147.774841\n",
      "Batch 653/2506: Loss 160.014236\n",
      "Batch 654/2506: Loss 66.775948\n",
      "Batch 655/2506: Loss 40.561142\n",
      "Batch 656/2506: Loss 85.753021\n",
      "Batch 657/2506: Loss 89.039368\n",
      "Batch 658/2506: Loss 45.209297\n",
      "Batch 659/2506: Loss 38.298592\n",
      "Batch 660/2506: Loss 60.688732\n",
      "Batch 661/2506: Loss 41.710018\n",
      "Batch 662/2506: Loss 54.487206\n",
      "Batch 663/2506: Loss 109.511200\n",
      "Batch 664/2506: Loss 48.585808\n",
      "Batch 665/2506: Loss 45.281776\n",
      "Batch 666/2506: Loss 41.711567\n",
      "Batch 667/2506: Loss 52.699726\n",
      "Batch 668/2506: Loss 109.026093\n",
      "Batch 669/2506: Loss 79.512154\n",
      "Batch 670/2506: Loss 151.465958\n",
      "Batch 671/2506: Loss 91.744598\n",
      "Batch 672/2506: Loss 80.052429\n",
      "Batch 673/2506: Loss 85.820534\n",
      "Batch 674/2506: Loss 70.632111\n",
      "Batch 675/2506: Loss 69.175568\n",
      "Batch 676/2506: Loss 30.173296\n",
      "Batch 677/2506: Loss 119.202560\n",
      "Batch 678/2506: Loss 50.328781\n",
      "Batch 679/2506: Loss 84.004242\n",
      "Batch 680/2506: Loss 39.970943\n",
      "Batch 681/2506: Loss 92.060318\n",
      "Batch 682/2506: Loss 102.096970\n",
      "Batch 683/2506: Loss 78.237312\n",
      "Batch 684/2506: Loss 37.065834\n",
      "Batch 685/2506: Loss 79.438560\n",
      "Batch 686/2506: Loss 82.009270\n",
      "Batch 687/2506: Loss 124.699226\n",
      "Batch 688/2506: Loss 56.996639\n",
      "Batch 689/2506: Loss 108.237785\n",
      "Batch 690/2506: Loss 36.983135\n",
      "Batch 691/2506: Loss 68.722855\n",
      "Batch 692/2506: Loss 93.680771\n",
      "Batch 693/2506: Loss 106.582947\n",
      "Batch 694/2506: Loss 55.313805\n",
      "Batch 695/2506: Loss 79.198273\n",
      "Batch 696/2506: Loss 178.913269\n",
      "Batch 697/2506: Loss 72.029442\n",
      "Batch 698/2506: Loss 83.903625\n",
      "Batch 699/2506: Loss 109.718712\n",
      "Batch 700/2506: Loss 96.402969\n",
      "Batch 701/2506: Loss 38.585876\n",
      "Batch 702/2506: Loss 76.401566\n",
      "Batch 703/2506: Loss 93.838692\n",
      "Batch 704/2506: Loss 77.696953\n",
      "Batch 705/2506: Loss 93.333298\n",
      "Batch 706/2506: Loss 53.532669\n",
      "Batch 707/2506: Loss 81.820251\n",
      "Batch 708/2506: Loss 130.219772\n",
      "Batch 709/2506: Loss 96.941193\n",
      "Batch 710/2506: Loss 43.053535\n",
      "Batch 711/2506: Loss 67.853645\n",
      "Batch 712/2506: Loss 202.367310\n",
      "Batch 713/2506: Loss 29.919863\n",
      "Batch 714/2506: Loss 74.043945\n",
      "Batch 715/2506: Loss 69.944870\n",
      "Batch 716/2506: Loss 47.709137\n",
      "Batch 717/2506: Loss 75.943771\n",
      "Batch 718/2506: Loss 48.821796\n",
      "Batch 719/2506: Loss 65.750992\n",
      "Batch 720/2506: Loss 80.718231\n",
      "Batch 721/2506: Loss 37.004799\n",
      "Batch 722/2506: Loss 68.917053\n",
      "Batch 723/2506: Loss 100.163567\n",
      "Batch 724/2506: Loss 63.062035\n",
      "Batch 725/2506: Loss 114.050331\n",
      "Batch 726/2506: Loss 67.340118\n",
      "Batch 727/2506: Loss 121.418793\n",
      "Batch 728/2506: Loss 87.513588\n",
      "Batch 729/2506: Loss 145.203232\n",
      "Batch 730/2506: Loss 138.885666\n",
      "Batch 731/2506: Loss 106.643936\n",
      "Batch 732/2506: Loss 99.671425\n",
      "Batch 733/2506: Loss 55.585312\n",
      "Batch 734/2506: Loss 72.867371\n",
      "Batch 735/2506: Loss 102.976624\n",
      "Batch 736/2506: Loss 65.019714\n",
      "Batch 737/2506: Loss 61.226021\n",
      "Batch 738/2506: Loss 55.529419\n",
      "Batch 739/2506: Loss 57.269325\n",
      "Batch 740/2506: Loss 69.167175\n",
      "Batch 741/2506: Loss 30.634743\n",
      "Batch 742/2506: Loss 73.819321\n",
      "Batch 743/2506: Loss 106.701218\n",
      "Batch 744/2506: Loss 56.409927\n",
      "Batch 745/2506: Loss 56.250557\n",
      "Batch 746/2506: Loss 103.261200\n",
      "Batch 747/2506: Loss 45.906300\n",
      "Batch 748/2506: Loss 99.706367\n",
      "Batch 749/2506: Loss 55.469093\n",
      "Batch 750/2506: Loss 109.163666\n",
      "Batch 751/2506: Loss 103.898788\n",
      "Batch 752/2506: Loss 41.697586\n",
      "Batch 753/2506: Loss 84.751251\n",
      "Batch 754/2506: Loss 150.770508\n",
      "Batch 755/2506: Loss 97.780380\n",
      "Batch 756/2506: Loss 78.819229\n",
      "Batch 757/2506: Loss 66.027946\n",
      "Batch 758/2506: Loss 111.142601\n",
      "Batch 759/2506: Loss 107.769096\n",
      "Batch 760/2506: Loss 130.867523\n",
      "Batch 761/2506: Loss 113.300117\n",
      "Batch 762/2506: Loss 49.819508\n",
      "Batch 763/2506: Loss 100.910179\n",
      "Batch 764/2506: Loss 42.835396\n",
      "Batch 765/2506: Loss 70.772591\n",
      "Batch 766/2506: Loss 47.124611\n",
      "Batch 767/2506: Loss 89.466156\n",
      "Batch 768/2506: Loss 40.538513\n",
      "Batch 769/2506: Loss 133.338806\n",
      "Batch 770/2506: Loss 75.579659\n",
      "Batch 771/2506: Loss 58.068218\n",
      "Batch 772/2506: Loss 85.716377\n",
      "Batch 773/2506: Loss 62.264278\n",
      "Batch 774/2506: Loss 35.327389\n",
      "Batch 775/2506: Loss 87.025146\n",
      "Batch 776/2506: Loss 45.601715\n",
      "Batch 777/2506: Loss 88.232788\n",
      "Batch 778/2506: Loss 52.935566\n",
      "Batch 779/2506: Loss 51.666611\n",
      "Batch 780/2506: Loss 51.360458\n",
      "Batch 781/2506: Loss 75.226410\n",
      "Batch 782/2506: Loss 58.157383\n",
      "Batch 783/2506: Loss 122.551125\n",
      "Batch 784/2506: Loss 69.165871\n",
      "Batch 785/2506: Loss 92.639412\n",
      "Batch 786/2506: Loss 135.338867\n",
      "Batch 787/2506: Loss 53.872894\n",
      "Batch 788/2506: Loss 72.341591\n",
      "Batch 789/2506: Loss 36.669456\n",
      "Batch 790/2506: Loss 121.011871\n",
      "Batch 791/2506: Loss 67.696495\n",
      "Batch 792/2506: Loss 65.913177\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 793/2506: Loss 43.018063\n",
      "Batch 794/2506: Loss 38.452492\n",
      "Batch 795/2506: Loss 80.181580\n",
      "Batch 796/2506: Loss 36.779701\n",
      "Batch 797/2506: Loss 29.834206\n",
      "Batch 798/2506: Loss 56.761375\n",
      "Batch 799/2506: Loss 37.981735\n",
      "Batch 800/2506: Loss 53.836891\n",
      "Batch 801/2506: Loss 69.883469\n",
      "Batch 802/2506: Loss 111.088150\n",
      "Batch 803/2506: Loss 139.586090\n",
      "Batch 804/2506: Loss 30.842525\n",
      "Batch 805/2506: Loss 68.417542\n",
      "Batch 806/2506: Loss 110.791069\n",
      "Batch 807/2506: Loss 72.501961\n",
      "Batch 808/2506: Loss 56.131218\n",
      "Batch 809/2506: Loss 74.805351\n",
      "Batch 810/2506: Loss 76.091736\n",
      "Batch 811/2506: Loss 31.769049\n",
      "Batch 812/2506: Loss 58.223461\n",
      "Batch 813/2506: Loss 99.151031\n",
      "Batch 814/2506: Loss 61.298836\n",
      "Batch 815/2506: Loss 62.344406\n",
      "Batch 816/2506: Loss 37.748604\n",
      "Batch 817/2506: Loss 77.727890\n",
      "Batch 818/2506: Loss 84.253975\n",
      "Batch 819/2506: Loss 43.455719\n",
      "Batch 820/2506: Loss 90.433502\n",
      "Batch 821/2506: Loss 104.507858\n",
      "Batch 822/2506: Loss 74.304123\n",
      "Batch 823/2506: Loss 65.402000\n",
      "Batch 824/2506: Loss 64.989487\n",
      "Batch 825/2506: Loss 32.388023\n",
      "Batch 826/2506: Loss 48.848270\n",
      "Batch 827/2506: Loss 106.278656\n",
      "Batch 828/2506: Loss 63.756371\n",
      "Batch 829/2506: Loss 110.970459\n",
      "Batch 830/2506: Loss 67.243019\n",
      "Batch 831/2506: Loss 51.572548\n",
      "Batch 832/2506: Loss 32.195042\n",
      "Batch 833/2506: Loss 75.906761\n",
      "Batch 834/2506: Loss 78.578888\n",
      "Batch 835/2506: Loss 42.132439\n",
      "Batch 836/2506: Loss 56.413891\n",
      "Batch 837/2506: Loss 55.358028\n",
      "Batch 838/2506: Loss 79.448013\n",
      "Batch 839/2506: Loss 48.182251\n",
      "Batch 840/2506: Loss 86.230331\n",
      "Batch 841/2506: Loss 31.538342\n",
      "Batch 842/2506: Loss 55.192905\n",
      "Batch 843/2506: Loss 35.679077\n",
      "Batch 844/2506: Loss 50.398731\n",
      "Batch 845/2506: Loss 34.471466\n",
      "Batch 846/2506: Loss 54.817913\n",
      "Batch 847/2506: Loss 63.177052\n",
      "Batch 848/2506: Loss 27.327877\n",
      "Batch 849/2506: Loss 80.901886\n",
      "Batch 850/2506: Loss 39.496479\n",
      "Batch 851/2506: Loss 103.679817\n",
      "Batch 852/2506: Loss 104.976669\n",
      "Batch 853/2506: Loss 57.977798\n",
      "Batch 854/2506: Loss 141.991043\n",
      "Batch 855/2506: Loss 43.133606\n",
      "Batch 856/2506: Loss 35.647861\n",
      "Batch 857/2506: Loss 58.694546\n",
      "Batch 858/2506: Loss 90.670860\n",
      "Batch 859/2506: Loss 60.964367\n",
      "Batch 860/2506: Loss 28.484135\n",
      "Batch 861/2506: Loss 52.366646\n",
      "Batch 862/2506: Loss 129.239975\n",
      "Batch 863/2506: Loss 48.377975\n",
      "Batch 864/2506: Loss 53.758518\n",
      "Batch 865/2506: Loss 57.174129\n",
      "Batch 866/2506: Loss 93.025337\n",
      "Batch 867/2506: Loss 62.902802\n",
      "Batch 868/2506: Loss 83.150566\n",
      "Batch 869/2506: Loss 31.705019\n",
      "Batch 870/2506: Loss 36.608932\n",
      "Batch 871/2506: Loss 126.668167\n",
      "Batch 872/2506: Loss 58.638798\n",
      "Batch 873/2506: Loss 111.472862\n",
      "Batch 874/2506: Loss 92.981117\n",
      "Batch 875/2506: Loss 122.634857\n",
      "Batch 876/2506: Loss 111.817017\n",
      "Batch 877/2506: Loss 69.411469\n",
      "Batch 878/2506: Loss 48.132149\n",
      "Batch 879/2506: Loss 104.752281\n",
      "Batch 880/2506: Loss 43.539818\n",
      "Batch 881/2506: Loss 140.787292\n",
      "Batch 882/2506: Loss 71.192497\n",
      "Batch 883/2506: Loss 89.978363\n",
      "Batch 884/2506: Loss 58.861057\n",
      "Batch 885/2506: Loss 54.621395\n",
      "Batch 886/2506: Loss 69.870811\n",
      "Batch 887/2506: Loss 213.800903\n",
      "Batch 888/2506: Loss 59.439880\n",
      "Batch 889/2506: Loss 69.611290\n",
      "Batch 890/2506: Loss 42.204189\n",
      "Batch 891/2506: Loss 65.688934\n",
      "Batch 892/2506: Loss 104.476753\n",
      "Batch 893/2506: Loss 129.280014\n",
      "Batch 894/2506: Loss 38.915680\n",
      "Batch 895/2506: Loss 46.105751\n",
      "Batch 896/2506: Loss 103.388443\n",
      "Batch 897/2506: Loss 44.435413\n",
      "Batch 898/2506: Loss 90.681320\n",
      "Batch 899/2506: Loss 38.895866\n",
      "Batch 900/2506: Loss 43.608475\n",
      "Batch 901/2506: Loss 151.215515\n",
      "Batch 902/2506: Loss 207.792542\n",
      "Batch 903/2506: Loss 159.013794\n",
      "Batch 904/2506: Loss 95.099388\n",
      "Batch 905/2506: Loss 199.204224\n",
      "Batch 906/2506: Loss 74.287262\n",
      "Batch 907/2506: Loss 93.823074\n",
      "Batch 908/2506: Loss 126.697418\n",
      "Batch 909/2506: Loss 89.945374\n",
      "Batch 910/2506: Loss 66.824860\n",
      "Batch 911/2506: Loss 88.835152\n",
      "Batch 912/2506: Loss 44.420380\n",
      "Batch 913/2506: Loss 59.271290\n",
      "Batch 914/2506: Loss 57.501625\n",
      "Batch 915/2506: Loss 98.206841\n",
      "Batch 916/2506: Loss 35.263134\n",
      "Batch 917/2506: Loss 90.433556\n",
      "Batch 918/2506: Loss 100.762268\n",
      "Batch 919/2506: Loss 55.204124\n",
      "Batch 920/2506: Loss 29.050686\n",
      "Batch 921/2506: Loss 81.961617\n",
      "Batch 922/2506: Loss 68.795715\n",
      "Batch 923/2506: Loss 61.076740\n",
      "Batch 924/2506: Loss 53.080734\n",
      "Batch 925/2506: Loss 78.132713\n",
      "Batch 926/2506: Loss 62.713135\n",
      "Batch 927/2506: Loss 111.500748\n",
      "Batch 928/2506: Loss 31.900169\n",
      "Batch 929/2506: Loss 52.171829\n",
      "Batch 930/2506: Loss 91.084038\n",
      "Batch 931/2506: Loss 87.239975\n",
      "Batch 932/2506: Loss 71.595337\n",
      "Batch 933/2506: Loss 38.643833\n",
      "Batch 934/2506: Loss 105.209938\n",
      "Batch 935/2506: Loss 64.381363\n",
      "Batch 936/2506: Loss 74.926865\n",
      "Batch 937/2506: Loss 35.465794\n",
      "Batch 938/2506: Loss 57.821228\n",
      "Batch 939/2506: Loss 51.437954\n",
      "Batch 940/2506: Loss 48.138687\n",
      "Batch 941/2506: Loss 33.131237\n",
      "Batch 942/2506: Loss 80.762100\n",
      "Batch 943/2506: Loss 90.611305\n",
      "Batch 944/2506: Loss 103.390656\n",
      "Batch 945/2506: Loss 64.925659\n",
      "Batch 946/2506: Loss 42.048691\n",
      "Batch 947/2506: Loss 84.762123\n",
      "Batch 948/2506: Loss 54.563599\n",
      "Batch 949/2506: Loss 166.908646\n",
      "Batch 950/2506: Loss 132.661804\n",
      "Batch 951/2506: Loss 58.034840\n",
      "Batch 952/2506: Loss 78.507408\n",
      "Batch 953/2506: Loss 96.106750\n",
      "Batch 954/2506: Loss 78.783051\n",
      "Batch 955/2506: Loss 120.316620\n",
      "Batch 956/2506: Loss 64.807846\n",
      "Batch 957/2506: Loss 63.503853\n",
      "Batch 958/2506: Loss 92.453262\n",
      "Batch 959/2506: Loss 74.443207\n",
      "Batch 960/2506: Loss 79.975433\n",
      "Batch 961/2506: Loss 83.424759\n",
      "Batch 962/2506: Loss 98.495071\n",
      "Batch 963/2506: Loss 128.348221\n",
      "Batch 964/2506: Loss 91.762428\n",
      "Batch 965/2506: Loss 69.500595\n",
      "Batch 966/2506: Loss 91.827431\n",
      "Batch 967/2506: Loss 115.442276\n",
      "Batch 968/2506: Loss 56.370380\n",
      "Batch 969/2506: Loss 121.012535\n",
      "Batch 970/2506: Loss 104.071098\n",
      "Batch 971/2506: Loss 52.324913\n",
      "Batch 972/2506: Loss 124.124237\n",
      "Batch 973/2506: Loss 142.043518\n",
      "Batch 974/2506: Loss 47.006485\n",
      "Batch 975/2506: Loss 59.566017\n",
      "Batch 976/2506: Loss 71.484085\n",
      "Batch 977/2506: Loss 133.033279\n",
      "Batch 978/2506: Loss 95.774200\n",
      "Batch 979/2506: Loss 172.794952\n",
      "Batch 980/2506: Loss 105.603760\n",
      "Batch 981/2506: Loss 76.350937\n",
      "Batch 982/2506: Loss 66.392944\n",
      "Batch 983/2506: Loss 39.082092\n",
      "Batch 984/2506: Loss 34.151417\n",
      "Batch 985/2506: Loss 68.490059\n",
      "Batch 986/2506: Loss 64.969566\n",
      "Batch 987/2506: Loss 38.888168\n",
      "Batch 988/2506: Loss 60.597054\n",
      "Batch 989/2506: Loss 63.625797\n",
      "Batch 990/2506: Loss 204.135651\n",
      "Batch 991/2506: Loss 43.936646\n",
      "Batch 992/2506: Loss 72.150558\n",
      "Batch 993/2506: Loss 53.756504\n",
      "Batch 994/2506: Loss 62.832920\n",
      "Batch 995/2506: Loss 37.796532\n",
      "Batch 996/2506: Loss 115.056061\n",
      "Batch 997/2506: Loss 85.182831\n",
      "Batch 998/2506: Loss 64.326340\n",
      "Batch 999/2506: Loss 87.903122\n",
      "Batch 1000/2506: Loss 83.783066\n",
      "Batch 1001/2506: Loss 85.303444\n",
      "Batch 1002/2506: Loss 66.217712\n",
      "Batch 1003/2506: Loss 113.569496\n",
      "Batch 1004/2506: Loss 33.204552\n",
      "Batch 1005/2506: Loss 66.430252\n",
      "Batch 1006/2506: Loss 82.446831\n",
      "Batch 1007/2506: Loss 70.596008\n",
      "Batch 1008/2506: Loss 44.183586\n",
      "Batch 1009/2506: Loss 88.422333\n",
      "Batch 1010/2506: Loss 216.997528\n",
      "Batch 1011/2506: Loss 58.380566\n",
      "Batch 1012/2506: Loss 57.136642\n",
      "Batch 1013/2506: Loss 276.891937\n",
      "Batch 1014/2506: Loss 28.233868\n",
      "Batch 1015/2506: Loss 42.570732\n",
      "Batch 1016/2506: Loss 41.432331\n",
      "Batch 1017/2506: Loss 78.842049\n",
      "Batch 1018/2506: Loss 74.914787\n",
      "Batch 1019/2506: Loss 63.017189\n",
      "Batch 1020/2506: Loss 87.866409\n",
      "Batch 1021/2506: Loss 134.013535\n",
      "Batch 1022/2506: Loss 36.791626\n",
      "Batch 1023/2506: Loss 46.704048\n",
      "Batch 1024/2506: Loss 92.500542\n",
      "Batch 1025/2506: Loss 50.987595\n",
      "Batch 1026/2506: Loss 127.827698\n",
      "Batch 1027/2506: Loss 57.810223\n",
      "Batch 1028/2506: Loss 60.643959\n",
      "Batch 1029/2506: Loss 242.290924\n",
      "Batch 1030/2506: Loss 130.109558\n",
      "Batch 1031/2506: Loss 37.332569\n",
      "Batch 1032/2506: Loss 92.384819\n",
      "Batch 1033/2506: Loss 37.909626\n",
      "Batch 1034/2506: Loss 110.604187\n",
      "Batch 1035/2506: Loss 28.909775\n",
      "Batch 1036/2506: Loss 151.357666\n",
      "Batch 1037/2506: Loss 105.084175\n",
      "Batch 1038/2506: Loss 49.437622\n",
      "Batch 1039/2506: Loss 148.835907\n",
      "Batch 1040/2506: Loss 95.060623\n",
      "Batch 1041/2506: Loss 44.252327\n",
      "Batch 1042/2506: Loss 28.475449\n",
      "Batch 1043/2506: Loss 41.519783\n",
      "Batch 1044/2506: Loss 94.869781\n",
      "Batch 1045/2506: Loss 44.705959\n",
      "Batch 1046/2506: Loss 54.928902\n",
      "Batch 1047/2506: Loss 45.049023\n",
      "Batch 1048/2506: Loss 61.479370\n",
      "Batch 1049/2506: Loss 59.858585\n",
      "Batch 1050/2506: Loss 74.571526\n",
      "Batch 1051/2506: Loss 126.287292\n",
      "Batch 1052/2506: Loss 41.356030\n",
      "Batch 1053/2506: Loss 118.869003\n",
      "Batch 1054/2506: Loss 49.144127\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1055/2506: Loss 42.324478\n",
      "Batch 1056/2506: Loss 95.858109\n",
      "Batch 1057/2506: Loss 88.060898\n",
      "Batch 1058/2506: Loss 69.274948\n",
      "Batch 1059/2506: Loss 140.874924\n",
      "Batch 1060/2506: Loss 44.345860\n",
      "Batch 1061/2506: Loss 63.483280\n",
      "Batch 1062/2506: Loss 63.534939\n",
      "Batch 1063/2506: Loss 128.690414\n",
      "Batch 1064/2506: Loss 110.754410\n",
      "Batch 1065/2506: Loss 39.196800\n",
      "Batch 1066/2506: Loss 48.566360\n",
      "Batch 1067/2506: Loss 105.348122\n",
      "Batch 1068/2506: Loss 80.876251\n",
      "Batch 1069/2506: Loss 100.856728\n",
      "Batch 1070/2506: Loss 37.938110\n",
      "Batch 1071/2506: Loss 51.584038\n",
      "Batch 1072/2506: Loss 49.077271\n",
      "Batch 1073/2506: Loss 103.273659\n",
      "Batch 1074/2506: Loss 151.765701\n",
      "Batch 1075/2506: Loss 99.773720\n",
      "Batch 1076/2506: Loss 61.662212\n",
      "Batch 1077/2506: Loss 55.350636\n",
      "Batch 1078/2506: Loss 112.907883\n",
      "Batch 1079/2506: Loss 87.386452\n",
      "Batch 1080/2506: Loss 50.048431\n",
      "Batch 1081/2506: Loss 33.008141\n",
      "Batch 1082/2506: Loss 75.391754\n",
      "Batch 1083/2506: Loss 41.007149\n",
      "Batch 1084/2506: Loss 40.653427\n",
      "Batch 1085/2506: Loss 65.886414\n",
      "Batch 1086/2506: Loss 143.792236\n",
      "Batch 1087/2506: Loss 51.328979\n",
      "Batch 1088/2506: Loss 72.685226\n",
      "Batch 1089/2506: Loss 140.643005\n",
      "Batch 1090/2506: Loss 57.282387\n",
      "Batch 1091/2506: Loss 47.234131\n",
      "Batch 1092/2506: Loss 37.488560\n",
      "Batch 1093/2506: Loss 83.327713\n",
      "Batch 1094/2506: Loss 63.322826\n",
      "Batch 1095/2506: Loss 152.756775\n",
      "Batch 1096/2506: Loss 61.757271\n",
      "Batch 1097/2506: Loss 55.876728\n",
      "Batch 1098/2506: Loss 93.458435\n",
      "Batch 1099/2506: Loss 84.190086\n",
      "Batch 1100/2506: Loss 69.459160\n",
      "Batch 1101/2506: Loss 34.685139\n",
      "Batch 1102/2506: Loss 37.231400\n",
      "Batch 1103/2506: Loss 37.407784\n",
      "Batch 1104/2506: Loss 53.131546\n",
      "Batch 1105/2506: Loss 48.316849\n",
      "Batch 1106/2506: Loss 82.041359\n",
      "Batch 1107/2506: Loss 43.646645\n",
      "Batch 1108/2506: Loss 33.927818\n",
      "Batch 1109/2506: Loss 29.594688\n",
      "Batch 1110/2506: Loss 54.282383\n",
      "Batch 1111/2506: Loss 78.473412\n",
      "Batch 1112/2506: Loss 76.737350\n",
      "Batch 1113/2506: Loss 70.249207\n",
      "Batch 1114/2506: Loss 36.134464\n",
      "Batch 1115/2506: Loss 47.600254\n",
      "Batch 1116/2506: Loss 97.929886\n",
      "Batch 1117/2506: Loss 52.047729\n",
      "Batch 1118/2506: Loss 57.626137\n",
      "Batch 1119/2506: Loss 67.565155\n",
      "Batch 1120/2506: Loss 66.522980\n",
      "Batch 1121/2506: Loss 46.389748\n",
      "Batch 1122/2506: Loss 55.372360\n",
      "Batch 1123/2506: Loss 53.593113\n",
      "Batch 1124/2506: Loss 86.179123\n",
      "Batch 1125/2506: Loss 43.518856\n",
      "Batch 1126/2506: Loss 136.443970\n",
      "Batch 1127/2506: Loss 46.549305\n",
      "Batch 1128/2506: Loss 106.043167\n",
      "Batch 1129/2506: Loss 50.139000\n",
      "Batch 1130/2506: Loss 59.546711\n",
      "Batch 1131/2506: Loss 166.072327\n",
      "Batch 1132/2506: Loss 63.036568\n",
      "Batch 1133/2506: Loss 103.966949\n",
      "Batch 1134/2506: Loss 140.258377\n",
      "Batch 1135/2506: Loss 68.584221\n",
      "Batch 1136/2506: Loss 99.794113\n",
      "Batch 1137/2506: Loss 32.479847\n",
      "Batch 1138/2506: Loss 425.494659\n",
      "Batch 1139/2506: Loss 98.717888\n",
      "Batch 1140/2506: Loss 80.383270\n",
      "Batch 1141/2506: Loss 103.712448\n",
      "Batch 1142/2506: Loss 38.562737\n",
      "Batch 1143/2506: Loss 57.137260\n",
      "Batch 1144/2506: Loss 104.256805\n",
      "Batch 1145/2506: Loss 62.563469\n",
      "Batch 1146/2506: Loss 85.803108\n",
      "Batch 1147/2506: Loss 76.909271\n",
      "Batch 1148/2506: Loss 60.911488\n",
      "Batch 1149/2506: Loss 53.884056\n",
      "Batch 1150/2506: Loss 47.674923\n",
      "Batch 1151/2506: Loss 101.117195\n",
      "Batch 1152/2506: Loss 63.091331\n",
      "Batch 1153/2506: Loss 69.522995\n",
      "Batch 1154/2506: Loss 97.057930\n",
      "Batch 1155/2506: Loss 44.112926\n",
      "Batch 1156/2506: Loss 127.430870\n",
      "Batch 1157/2506: Loss 112.908974\n",
      "Batch 1158/2506: Loss 55.995651\n",
      "Batch 1159/2506: Loss 69.854141\n",
      "Batch 1160/2506: Loss 59.223892\n",
      "Batch 1161/2506: Loss 86.624924\n",
      "Batch 1162/2506: Loss 64.520142\n",
      "Batch 1163/2506: Loss 51.665451\n",
      "Batch 1164/2506: Loss 46.021797\n",
      "Batch 1165/2506: Loss 63.343880\n",
      "Batch 1166/2506: Loss 39.936531\n",
      "Batch 1167/2506: Loss 114.924133\n",
      "Batch 1168/2506: Loss 37.610638\n",
      "Batch 1169/2506: Loss 167.212128\n",
      "Batch 1170/2506: Loss 77.685257\n",
      "Batch 1171/2506: Loss 44.333279\n",
      "Batch 1172/2506: Loss 192.379395\n",
      "Batch 1173/2506: Loss 74.623215\n",
      "Batch 1174/2506: Loss 118.783279\n",
      "Batch 1175/2506: Loss 32.312870\n",
      "Batch 1176/2506: Loss 79.516388\n",
      "Batch 1177/2506: Loss 31.902702\n",
      "Batch 1178/2506: Loss 70.472961\n",
      "Batch 1179/2506: Loss 54.211670\n",
      "Batch 1180/2506: Loss 73.578278\n",
      "Batch 1181/2506: Loss 59.310036\n",
      "Batch 1182/2506: Loss 63.435616\n",
      "Batch 1183/2506: Loss 137.213776\n",
      "Batch 1184/2506: Loss 56.670750\n",
      "Batch 1185/2506: Loss 80.704514\n",
      "Batch 1186/2506: Loss 40.813175\n",
      "Batch 1187/2506: Loss 112.080940\n",
      "Batch 1188/2506: Loss 30.596024\n",
      "Batch 1189/2506: Loss 38.699112\n",
      "Batch 1190/2506: Loss 65.690575\n",
      "Batch 1191/2506: Loss 35.180977\n",
      "Batch 1192/2506: Loss 58.337612\n",
      "Batch 1193/2506: Loss 72.409302\n",
      "Batch 1194/2506: Loss 62.181297\n",
      "Batch 1195/2506: Loss 127.773476\n",
      "Batch 1196/2506: Loss 39.774334\n",
      "Batch 1197/2506: Loss 39.309746\n",
      "Batch 1198/2506: Loss 30.635698\n",
      "Batch 1199/2506: Loss 85.503860\n",
      "Batch 1200/2506: Loss 269.190308\n",
      "Batch 1201/2506: Loss 96.268295\n",
      "Batch 1202/2506: Loss 49.529785\n",
      "Batch 1203/2506: Loss 68.558449\n",
      "Batch 1204/2506: Loss 128.052979\n",
      "Batch 1205/2506: Loss 71.905228\n",
      "Batch 1206/2506: Loss 56.393085\n",
      "Batch 1207/2506: Loss 106.965797\n",
      "Batch 1208/2506: Loss 46.438625\n",
      "Batch 1209/2506: Loss 80.647911\n",
      "Batch 1210/2506: Loss 29.132570\n",
      "Batch 1211/2506: Loss 125.551109\n",
      "Batch 1212/2506: Loss 192.895920\n",
      "Batch 1213/2506: Loss 45.668488\n",
      "Batch 1214/2506: Loss 118.818024\n",
      "Batch 1215/2506: Loss 74.873123\n",
      "Batch 1216/2506: Loss 51.667713\n",
      "Batch 1217/2506: Loss 94.300148\n",
      "Batch 1218/2506: Loss 71.184494\n",
      "Batch 1219/2506: Loss 72.638016\n",
      "Batch 1220/2506: Loss 86.875961\n",
      "Batch 1221/2506: Loss 161.854736\n",
      "Batch 1222/2506: Loss 87.405937\n",
      "Batch 1223/2506: Loss 255.839767\n",
      "Batch 1224/2506: Loss 54.605782\n",
      "Batch 1225/2506: Loss 85.244171\n",
      "Batch 1226/2506: Loss 78.272949\n",
      "Batch 1227/2506: Loss 30.553917\n",
      "Batch 1228/2506: Loss 85.343994\n",
      "Batch 1229/2506: Loss 113.183578\n",
      "Batch 1230/2506: Loss 54.736092\n",
      "Batch 1231/2506: Loss 70.197540\n",
      "Batch 1232/2506: Loss 59.581539\n",
      "Batch 1233/2506: Loss 74.609016\n",
      "Batch 1234/2506: Loss 77.347244\n",
      "Batch 1235/2506: Loss 75.957031\n",
      "Batch 1236/2506: Loss 110.133972\n",
      "Batch 1237/2506: Loss 67.884064\n",
      "Batch 1238/2506: Loss 175.015213\n",
      "Batch 1239/2506: Loss 72.282768\n",
      "Batch 1240/2506: Loss 32.761990\n",
      "Batch 1241/2506: Loss 90.389267\n",
      "Batch 1242/2506: Loss 184.169250\n",
      "Batch 1243/2506: Loss 80.585770\n",
      "Batch 1244/2506: Loss 38.151981\n",
      "Batch 1245/2506: Loss 93.156120\n",
      "Batch 1246/2506: Loss 148.258835\n",
      "Batch 1247/2506: Loss 88.081528\n",
      "Batch 1248/2506: Loss 97.003784\n",
      "Batch 1249/2506: Loss 43.100922\n",
      "Batch 1250/2506: Loss 43.550827\n",
      "Batch 1251/2506: Loss 106.439018\n",
      "Batch 1252/2506: Loss 71.242241\n",
      "Batch 1253/2506: Loss 97.680405\n",
      "Batch 1254/2506: Loss 64.390320\n",
      "Batch 1255/2506: Loss 99.142929\n",
      "Batch 1256/2506: Loss 52.090187\n",
      "Batch 1257/2506: Loss 126.481628\n",
      "Batch 1258/2506: Loss 29.721889\n",
      "Batch 1259/2506: Loss 149.573410\n",
      "Batch 1260/2506: Loss 99.915039\n",
      "Batch 1261/2506: Loss 41.568668\n",
      "Batch 1262/2506: Loss 81.972855\n",
      "Batch 1263/2506: Loss 84.342712\n",
      "Batch 1264/2506: Loss 50.544731\n",
      "Batch 1265/2506: Loss 167.883011\n",
      "Batch 1266/2506: Loss 98.405716\n",
      "Batch 1267/2506: Loss 110.756599\n",
      "Batch 1268/2506: Loss 59.728165\n",
      "Batch 1269/2506: Loss 65.235718\n",
      "Batch 1270/2506: Loss 82.275848\n",
      "Batch 1271/2506: Loss 78.133957\n",
      "Batch 1272/2506: Loss 66.571564\n",
      "Batch 1273/2506: Loss 113.681862\n",
      "Batch 1274/2506: Loss 37.538433\n",
      "Batch 1275/2506: Loss 104.633179\n",
      "Batch 1276/2506: Loss 55.795544\n",
      "Batch 1277/2506: Loss 109.891220\n",
      "Batch 1278/2506: Loss 49.155666\n",
      "Batch 1279/2506: Loss 67.584190\n",
      "Batch 1280/2506: Loss 63.110817\n",
      "Batch 1281/2506: Loss 98.080864\n",
      "Batch 1282/2506: Loss 65.187630\n",
      "Batch 1283/2506: Loss 51.884327\n",
      "Batch 1284/2506: Loss 49.026340\n",
      "Batch 1285/2506: Loss 95.214409\n",
      "Batch 1286/2506: Loss 55.446678\n",
      "Batch 1287/2506: Loss 143.528748\n",
      "Batch 1288/2506: Loss 104.237679\n",
      "Batch 1289/2506: Loss 118.467110\n",
      "Batch 1290/2506: Loss 58.934204\n",
      "Batch 1291/2506: Loss 42.706127\n",
      "Batch 1292/2506: Loss 99.602737\n",
      "Batch 1293/2506: Loss 49.979168\n",
      "Batch 1294/2506: Loss 53.541679\n",
      "Batch 1295/2506: Loss 30.634209\n",
      "Batch 1296/2506: Loss 66.308022\n",
      "Batch 1297/2506: Loss 58.709038\n",
      "Batch 1298/2506: Loss 69.010651\n",
      "Batch 1299/2506: Loss 71.251129\n",
      "Batch 1300/2506: Loss 167.750519\n",
      "Batch 1301/2506: Loss 41.105270\n",
      "Batch 1302/2506: Loss 53.547886\n",
      "Batch 1303/2506: Loss 73.562180\n",
      "Batch 1304/2506: Loss 50.854210\n",
      "Batch 1305/2506: Loss 41.342049\n",
      "Batch 1306/2506: Loss 79.733444\n",
      "Batch 1307/2506: Loss 111.287277\n",
      "Batch 1308/2506: Loss 85.066956\n",
      "Batch 1309/2506: Loss 91.661064\n",
      "Batch 1310/2506: Loss 71.302979\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1311/2506: Loss 30.878880\n",
      "Batch 1312/2506: Loss 77.042839\n",
      "Batch 1313/2506: Loss 81.761749\n",
      "Batch 1314/2506: Loss 98.839302\n",
      "Batch 1315/2506: Loss 89.260582\n",
      "Batch 1316/2506: Loss 111.548294\n",
      "Batch 1317/2506: Loss 51.338398\n",
      "Batch 1318/2506: Loss 90.543671\n",
      "Batch 1319/2506: Loss 76.143318\n",
      "Batch 1320/2506: Loss 44.906372\n",
      "Batch 1321/2506: Loss 63.128101\n",
      "Batch 1322/2506: Loss 192.604111\n",
      "Batch 1323/2506: Loss 205.264694\n",
      "Batch 1324/2506: Loss 45.434921\n",
      "Batch 1325/2506: Loss 145.444000\n",
      "Batch 1326/2506: Loss 202.293854\n",
      "Batch 1327/2506: Loss 51.147419\n",
      "Batch 1328/2506: Loss 113.965736\n",
      "Batch 1329/2506: Loss 98.439476\n",
      "Batch 1330/2506: Loss 65.788330\n",
      "Batch 1331/2506: Loss 110.123451\n",
      "Batch 1332/2506: Loss 96.730141\n",
      "Batch 1333/2506: Loss 77.445145\n",
      "Batch 1334/2506: Loss 50.380577\n",
      "Batch 1335/2506: Loss 130.461243\n",
      "Batch 1336/2506: Loss 47.734318\n",
      "Batch 1337/2506: Loss 39.068459\n",
      "Batch 1338/2506: Loss 185.366669\n",
      "Batch 1339/2506: Loss 75.721718\n",
      "Batch 1340/2506: Loss 61.327587\n",
      "Batch 1341/2506: Loss 60.528404\n",
      "Batch 1342/2506: Loss 47.233376\n",
      "Batch 1343/2506: Loss 133.525696\n",
      "Batch 1344/2506: Loss 74.367264\n",
      "Batch 1345/2506: Loss 89.540039\n",
      "Batch 1346/2506: Loss 84.067551\n",
      "Batch 1347/2506: Loss 79.854156\n",
      "Batch 1348/2506: Loss 68.564011\n",
      "Batch 1349/2506: Loss 26.135017\n",
      "Batch 1350/2506: Loss 35.250160\n",
      "Batch 1351/2506: Loss 36.773720\n",
      "Batch 1352/2506: Loss 195.311798\n",
      "Batch 1353/2506: Loss 38.565239\n",
      "Batch 1354/2506: Loss 83.896515\n",
      "Batch 1355/2506: Loss 148.236725\n",
      "Batch 1356/2506: Loss 65.995399\n",
      "Batch 1357/2506: Loss 53.429958\n",
      "Batch 1358/2506: Loss 77.436996\n",
      "Batch 1359/2506: Loss 74.426208\n",
      "Batch 1360/2506: Loss 63.763130\n",
      "Batch 1361/2506: Loss 68.543945\n",
      "Batch 1362/2506: Loss 49.023552\n",
      "Batch 1363/2506: Loss 68.206619\n",
      "Batch 1364/2506: Loss 77.763565\n",
      "Batch 1365/2506: Loss 96.664650\n",
      "Batch 1366/2506: Loss 132.083008\n",
      "Batch 1367/2506: Loss 126.729607\n",
      "Batch 1368/2506: Loss 82.465462\n",
      "Batch 1369/2506: Loss 33.315052\n",
      "Batch 1370/2506: Loss 29.222176\n",
      "Batch 1371/2506: Loss 116.854103\n",
      "Batch 1372/2506: Loss 39.646652\n",
      "Batch 1373/2506: Loss 42.328613\n",
      "Batch 1374/2506: Loss 46.715984\n",
      "Batch 1375/2506: Loss 79.847099\n",
      "Batch 1376/2506: Loss 48.988194\n",
      "Batch 1377/2506: Loss 69.328232\n",
      "Batch 1378/2506: Loss 50.884621\n",
      "Batch 1379/2506: Loss 78.694672\n",
      "Batch 1380/2506: Loss 80.499878\n",
      "Batch 1381/2506: Loss 68.086617\n",
      "Batch 1382/2506: Loss 37.042282\n",
      "Batch 1383/2506: Loss 65.516617\n",
      "Batch 1384/2506: Loss 80.151665\n",
      "Batch 1385/2506: Loss 79.703918\n",
      "Batch 1386/2506: Loss 120.312241\n",
      "Batch 1387/2506: Loss 96.472939\n",
      "Batch 1388/2506: Loss 65.408165\n",
      "Batch 1389/2506: Loss 57.423943\n",
      "Batch 1390/2506: Loss 63.303085\n",
      "Batch 1391/2506: Loss 45.194592\n",
      "Batch 1392/2506: Loss 52.670597\n",
      "Batch 1393/2506: Loss 90.472008\n",
      "Batch 1394/2506: Loss 45.590698\n",
      "Batch 1395/2506: Loss 93.131790\n",
      "Batch 1396/2506: Loss 60.571316\n",
      "Batch 1397/2506: Loss 65.538330\n",
      "Batch 1398/2506: Loss 88.744492\n",
      "Batch 1399/2506: Loss 55.300392\n",
      "Batch 1400/2506: Loss 93.434830\n",
      "Batch 1401/2506: Loss 72.696854\n",
      "Batch 1402/2506: Loss 122.063339\n",
      "Batch 1403/2506: Loss 57.551125\n",
      "Batch 1404/2506: Loss 39.858002\n",
      "Batch 1405/2506: Loss 93.344414\n",
      "Batch 1406/2506: Loss 104.162796\n",
      "Batch 1407/2506: Loss 46.355225\n",
      "Batch 1408/2506: Loss 91.640442\n",
      "Batch 1409/2506: Loss 119.401184\n",
      "Batch 1410/2506: Loss 54.368465\n",
      "Batch 1411/2506: Loss 101.820503\n",
      "Batch 1412/2506: Loss 123.922119\n",
      "Batch 1413/2506: Loss 48.441742\n",
      "Batch 1414/2506: Loss 52.301636\n",
      "Batch 1415/2506: Loss 35.303230\n",
      "Batch 1416/2506: Loss 86.038383\n",
      "Batch 1417/2506: Loss 74.735451\n",
      "Batch 1418/2506: Loss 96.839012\n",
      "Batch 1419/2506: Loss 81.841904\n",
      "Batch 1420/2506: Loss 56.375809\n",
      "Batch 1421/2506: Loss 28.395184\n",
      "Batch 1422/2506: Loss 28.969393\n",
      "Batch 1423/2506: Loss 118.279793\n",
      "Batch 1424/2506: Loss 89.990372\n",
      "Batch 1425/2506: Loss 45.725155\n",
      "Batch 1426/2506: Loss 86.784668\n",
      "Batch 1427/2506: Loss 92.171997\n",
      "Batch 1428/2506: Loss 32.382961\n",
      "Batch 1429/2506: Loss 40.165215\n",
      "Batch 1430/2506: Loss 119.186676\n",
      "Batch 1431/2506: Loss 146.506653\n",
      "Batch 1432/2506: Loss 38.410957\n",
      "Batch 1433/2506: Loss 114.818039\n",
      "Batch 1434/2506: Loss 40.480118\n",
      "Batch 1435/2506: Loss 56.233891\n",
      "Batch 1436/2506: Loss 89.598534\n",
      "Batch 1437/2506: Loss 28.996489\n",
      "Batch 1438/2506: Loss 113.692711\n",
      "Batch 1439/2506: Loss 30.945967\n",
      "Batch 1440/2506: Loss 81.223495\n",
      "Batch 1441/2506: Loss 52.371712\n",
      "Batch 1442/2506: Loss 41.945881\n",
      "Batch 1443/2506: Loss 105.759354\n",
      "Batch 1444/2506: Loss 174.229645\n",
      "Batch 1445/2506: Loss 41.297935\n",
      "Batch 1446/2506: Loss 50.540497\n",
      "Batch 1447/2506: Loss 50.780827\n",
      "Batch 1448/2506: Loss 93.769745\n",
      "Batch 1449/2506: Loss 61.009785\n",
      "Batch 1450/2506: Loss 67.868813\n",
      "Batch 1451/2506: Loss 34.208794\n",
      "Batch 1452/2506: Loss 93.892120\n",
      "Batch 1453/2506: Loss 47.277618\n",
      "Batch 1454/2506: Loss 100.511261\n",
      "Batch 1455/2506: Loss 48.247391\n",
      "Batch 1456/2506: Loss 101.650162\n",
      "Batch 1457/2506: Loss 135.610718\n",
      "Batch 1458/2506: Loss 156.436279\n",
      "Batch 1459/2506: Loss 87.213417\n",
      "Batch 1460/2506: Loss 76.081192\n",
      "Batch 1461/2506: Loss 196.325974\n",
      "Batch 1462/2506: Loss 46.298645\n",
      "Batch 1463/2506: Loss 42.384945\n",
      "Batch 1464/2506: Loss 82.006416\n",
      "Batch 1465/2506: Loss 30.279181\n",
      "Batch 1466/2506: Loss 119.067963\n",
      "Batch 1467/2506: Loss 83.033859\n",
      "Batch 1468/2506: Loss 65.884094\n",
      "Batch 1469/2506: Loss 101.348969\n",
      "Batch 1470/2506: Loss 73.575676\n",
      "Batch 1471/2506: Loss 151.256500\n",
      "Batch 1472/2506: Loss 60.870010\n",
      "Batch 1473/2506: Loss 76.823700\n",
      "Batch 1474/2506: Loss 80.497177\n",
      "Batch 1475/2506: Loss 87.353828\n",
      "Batch 1476/2506: Loss 55.905876\n",
      "Batch 1477/2506: Loss 90.023628\n",
      "Batch 1478/2506: Loss 52.545204\n",
      "Batch 1479/2506: Loss 169.495071\n",
      "Batch 1480/2506: Loss 105.260818\n",
      "Batch 1481/2506: Loss 112.205376\n",
      "Batch 1482/2506: Loss 130.983688\n",
      "Batch 1483/2506: Loss 120.631691\n",
      "Batch 1484/2506: Loss 53.337776\n",
      "Batch 1485/2506: Loss 72.921692\n",
      "Batch 1486/2506: Loss 143.700729\n",
      "Batch 1487/2506: Loss 68.094185\n",
      "Batch 1488/2506: Loss 86.257202\n",
      "Batch 1489/2506: Loss 52.938313\n",
      "Batch 1490/2506: Loss 43.531734\n",
      "Batch 1491/2506: Loss 33.148869\n",
      "Batch 1492/2506: Loss 89.334030\n",
      "Batch 1493/2506: Loss 90.181313\n",
      "Batch 1494/2506: Loss 36.628979\n",
      "Batch 1495/2506: Loss 86.606567\n",
      "Batch 1496/2506: Loss 41.102177\n",
      "Batch 1497/2506: Loss 81.727692\n",
      "Batch 1498/2506: Loss 38.081436\n",
      "Batch 1499/2506: Loss 63.077427\n",
      "Batch 1500/2506: Loss 68.976944\n",
      "Batch 1501/2506: Loss 91.071320\n",
      "Batch 1502/2506: Loss 140.914139\n",
      "Batch 1503/2506: Loss 91.712738\n",
      "Batch 1504/2506: Loss 126.237015\n",
      "Batch 1505/2506: Loss 52.202286\n",
      "Batch 1506/2506: Loss 44.991314\n",
      "Batch 1507/2506: Loss 86.026184\n",
      "Batch 1508/2506: Loss 88.012924\n",
      "Batch 1509/2506: Loss 105.085098\n",
      "Batch 1510/2506: Loss 56.104706\n",
      "Batch 1511/2506: Loss 101.776566\n",
      "Batch 1512/2506: Loss 65.411156\n",
      "Batch 1513/2506: Loss 67.532394\n",
      "Batch 1514/2506: Loss 66.814743\n",
      "Batch 1515/2506: Loss 68.644165\n",
      "Batch 1516/2506: Loss 49.769638\n",
      "Batch 1517/2506: Loss 111.165352\n",
      "Batch 1518/2506: Loss 63.750134\n",
      "Batch 1519/2506: Loss 70.569031\n",
      "Batch 1520/2506: Loss 71.385361\n",
      "Batch 1521/2506: Loss 78.144287\n",
      "Batch 1522/2506: Loss 70.670044\n",
      "Batch 1523/2506: Loss 41.460636\n",
      "Batch 1524/2506: Loss 56.333942\n",
      "Batch 1525/2506: Loss 96.590294\n",
      "Batch 1526/2506: Loss 51.978230\n",
      "Batch 1527/2506: Loss 44.177143\n",
      "Batch 1528/2506: Loss 52.280334\n",
      "Batch 1529/2506: Loss 67.398956\n",
      "Batch 1530/2506: Loss 59.750294\n",
      "Batch 1531/2506: Loss 67.129646\n",
      "Batch 1532/2506: Loss 131.215637\n",
      "Batch 1533/2506: Loss 119.678284\n",
      "Batch 1534/2506: Loss 145.820374\n",
      "Batch 1535/2506: Loss 57.092278\n",
      "Batch 1536/2506: Loss 138.534485\n",
      "Batch 1537/2506: Loss 181.372925\n",
      "Batch 1538/2506: Loss 70.836723\n",
      "Batch 1539/2506: Loss 68.820145\n",
      "Batch 1540/2506: Loss 52.318199\n",
      "Batch 1541/2506: Loss 53.436722\n",
      "Batch 1542/2506: Loss 96.209167\n",
      "Batch 1543/2506: Loss 63.513271\n",
      "Batch 1544/2506: Loss 60.023609\n",
      "Batch 1545/2506: Loss 72.123398\n",
      "Batch 1546/2506: Loss 127.016876\n",
      "Batch 1547/2506: Loss 112.560806\n",
      "Batch 1548/2506: Loss 210.614807\n",
      "Batch 1549/2506: Loss 55.097237\n",
      "Batch 1550/2506: Loss 31.572300\n",
      "Batch 1551/2506: Loss 84.379288\n",
      "Batch 1552/2506: Loss 32.477882\n",
      "Batch 1553/2506: Loss 40.189903\n",
      "Batch 1554/2506: Loss 86.338333\n",
      "Batch 1555/2506: Loss 55.449539\n",
      "Batch 1556/2506: Loss 62.332169\n",
      "Batch 1557/2506: Loss 107.811142\n",
      "Batch 1558/2506: Loss 162.819077\n",
      "Batch 1559/2506: Loss 50.040203\n",
      "Batch 1560/2506: Loss 89.857285\n",
      "Batch 1561/2506: Loss 88.923714\n",
      "Batch 1562/2506: Loss 36.321297\n",
      "Batch 1563/2506: Loss 52.840481\n",
      "Batch 1564/2506: Loss 69.724884\n",
      "Batch 1565/2506: Loss 56.521732\n",
      "Batch 1566/2506: Loss 79.463516\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1567/2506: Loss 97.401810\n",
      "Batch 1568/2506: Loss 116.283440\n",
      "Batch 1569/2506: Loss 35.287819\n",
      "Batch 1570/2506: Loss 73.864845\n",
      "Batch 1571/2506: Loss 49.474995\n",
      "Batch 1572/2506: Loss 67.821098\n",
      "Batch 1573/2506: Loss 50.035782\n",
      "Batch 1574/2506: Loss 93.619453\n",
      "Batch 1575/2506: Loss 93.406219\n",
      "Batch 1576/2506: Loss 141.969757\n",
      "Batch 1577/2506: Loss 45.722977\n",
      "Batch 1578/2506: Loss 95.844437\n",
      "Batch 1579/2506: Loss 58.391685\n",
      "Batch 1580/2506: Loss 33.688980\n",
      "Batch 1581/2506: Loss 78.935097\n",
      "Batch 1582/2506: Loss 77.258163\n",
      "Batch 1583/2506: Loss 57.500526\n",
      "Batch 1584/2506: Loss 60.568314\n",
      "Batch 1585/2506: Loss 33.124142\n",
      "Batch 1586/2506: Loss 84.137718\n",
      "Batch 1587/2506: Loss 60.237640\n",
      "Batch 1588/2506: Loss 47.055500\n",
      "Batch 1589/2506: Loss 51.871750\n",
      "Batch 1590/2506: Loss 96.617470\n",
      "Batch 1591/2506: Loss 69.691200\n",
      "Batch 1592/2506: Loss 73.333824\n",
      "Batch 1593/2506: Loss 50.217243\n",
      "Batch 1594/2506: Loss 147.232895\n",
      "Batch 1595/2506: Loss 95.361252\n",
      "Batch 1596/2506: Loss 48.563824\n",
      "Batch 1597/2506: Loss 51.425404\n",
      "Batch 1598/2506: Loss 68.993385\n",
      "Batch 1599/2506: Loss 54.253311\n",
      "Batch 1600/2506: Loss 142.908234\n",
      "Batch 1601/2506: Loss 42.346863\n",
      "Batch 1602/2506: Loss 46.093960\n",
      "Batch 1603/2506: Loss 84.093819\n",
      "Batch 1604/2506: Loss 100.520050\n",
      "Batch 1605/2506: Loss 66.961090\n",
      "Batch 1606/2506: Loss 61.527557\n",
      "Batch 1607/2506: Loss 30.184376\n",
      "Batch 1608/2506: Loss 105.419426\n",
      "Batch 1609/2506: Loss 49.517544\n",
      "Batch 1610/2506: Loss 134.180389\n",
      "Batch 1611/2506: Loss 83.621918\n",
      "Batch 1612/2506: Loss 49.810791\n",
      "Batch 1613/2506: Loss 78.969734\n",
      "Batch 1614/2506: Loss 56.847359\n",
      "Batch 1615/2506: Loss 175.954956\n",
      "Batch 1616/2506: Loss 82.158287\n",
      "Batch 1617/2506: Loss 40.569336\n",
      "Batch 1618/2506: Loss 81.302910\n",
      "Batch 1619/2506: Loss 66.611488\n",
      "Batch 1620/2506: Loss 83.187683\n",
      "Batch 1621/2506: Loss 58.706985\n",
      "Batch 1622/2506: Loss 39.383900\n",
      "Batch 1623/2506: Loss 98.102448\n",
      "Batch 1624/2506: Loss 94.344177\n",
      "Batch 1625/2506: Loss 57.823448\n",
      "Batch 1626/2506: Loss 40.664940\n",
      "Batch 1627/2506: Loss 73.853569\n",
      "Batch 1628/2506: Loss 96.149681\n",
      "Batch 1629/2506: Loss 112.826759\n",
      "Batch 1630/2506: Loss 84.081116\n",
      "Batch 1631/2506: Loss 42.964058\n",
      "Batch 1632/2506: Loss 52.051384\n",
      "Batch 1633/2506: Loss 75.939545\n",
      "Batch 1634/2506: Loss 90.061295\n",
      "Batch 1635/2506: Loss 57.609848\n",
      "Batch 1636/2506: Loss 89.089066\n",
      "Batch 1637/2506: Loss 98.018845\n",
      "Batch 1638/2506: Loss 74.536942\n",
      "Batch 1639/2506: Loss 43.717808\n",
      "Batch 1640/2506: Loss 88.848137\n",
      "Batch 1641/2506: Loss 51.319592\n",
      "Batch 1642/2506: Loss 78.634651\n",
      "Batch 1643/2506: Loss 53.513126\n",
      "Batch 1644/2506: Loss 110.756294\n",
      "Batch 1645/2506: Loss 103.044220\n",
      "Batch 1646/2506: Loss 61.090816\n",
      "Batch 1647/2506: Loss 64.997978\n",
      "Batch 1648/2506: Loss 73.022285\n",
      "Batch 1649/2506: Loss 111.854408\n",
      "Batch 1650/2506: Loss 73.305000\n",
      "Batch 1651/2506: Loss 69.404663\n",
      "Batch 1652/2506: Loss 41.547691\n",
      "Batch 1653/2506: Loss 149.870850\n",
      "Batch 1654/2506: Loss 37.250725\n",
      "Batch 1655/2506: Loss 48.610821\n",
      "Batch 1656/2506: Loss 97.702698\n",
      "Batch 1657/2506: Loss 83.644257\n",
      "Batch 1658/2506: Loss 77.436600\n",
      "Batch 1659/2506: Loss 99.817116\n",
      "Batch 1660/2506: Loss 68.760292\n",
      "Batch 1661/2506: Loss 75.022560\n",
      "Batch 1662/2506: Loss 31.515068\n",
      "Batch 1663/2506: Loss 82.916397\n",
      "Batch 1664/2506: Loss 98.515427\n",
      "Batch 1665/2506: Loss 39.607174\n",
      "Batch 1666/2506: Loss 86.013443\n",
      "Batch 1667/2506: Loss 153.469620\n",
      "Batch 1668/2506: Loss 130.974716\n",
      "Batch 1669/2506: Loss 64.998306\n",
      "Batch 1670/2506: Loss 54.418129\n",
      "Batch 1671/2506: Loss 109.356087\n",
      "Batch 1672/2506: Loss 43.909943\n",
      "Batch 1673/2506: Loss 49.276379\n",
      "Batch 1674/2506: Loss 82.813278\n",
      "Batch 1675/2506: Loss 63.130104\n",
      "Batch 1676/2506: Loss 81.107933\n",
      "Batch 1677/2506: Loss 77.630112\n",
      "Batch 1678/2506: Loss 114.883766\n",
      "Batch 1679/2506: Loss 72.244576\n",
      "Batch 1680/2506: Loss 100.944290\n",
      "Batch 1681/2506: Loss 54.626556\n",
      "Batch 1682/2506: Loss 85.641479\n",
      "Batch 1683/2506: Loss 95.070396\n",
      "Batch 1684/2506: Loss 62.461540\n",
      "Batch 1685/2506: Loss 77.368134\n",
      "Batch 1686/2506: Loss 81.986992\n",
      "Batch 1687/2506: Loss 69.743927\n",
      "Batch 1688/2506: Loss 64.330795\n",
      "Batch 1689/2506: Loss 44.490726\n",
      "Batch 1690/2506: Loss 147.415604\n",
      "Batch 1691/2506: Loss 77.742416\n",
      "Batch 1692/2506: Loss 74.927628\n",
      "Batch 1693/2506: Loss 42.690617\n",
      "Batch 1694/2506: Loss 80.382790\n",
      "Batch 1695/2506: Loss 100.030113\n",
      "Batch 1696/2506: Loss 187.228821\n",
      "Batch 1697/2506: Loss 30.317078\n",
      "Batch 1698/2506: Loss 54.075981\n",
      "Batch 1699/2506: Loss 38.816349\n",
      "Batch 1700/2506: Loss 103.066040\n",
      "Batch 1701/2506: Loss 31.101109\n",
      "Batch 1702/2506: Loss 70.731400\n",
      "Batch 1703/2506: Loss 71.845520\n",
      "Batch 1704/2506: Loss 45.934345\n",
      "Batch 1705/2506: Loss 89.213509\n",
      "Batch 1706/2506: Loss 110.170593\n",
      "Batch 1707/2506: Loss 127.336128\n",
      "Batch 1708/2506: Loss 103.026657\n",
      "Batch 1709/2506: Loss 56.772316\n",
      "Batch 1710/2506: Loss 88.640381\n",
      "Batch 1711/2506: Loss 66.476799\n",
      "Batch 1712/2506: Loss 102.826714\n",
      "Batch 1713/2506: Loss 59.454025\n",
      "Batch 1714/2506: Loss 140.677795\n",
      "Batch 1715/2506: Loss 123.889114\n",
      "Batch 1716/2506: Loss 159.218445\n",
      "Batch 1717/2506: Loss 85.055511\n",
      "Batch 1718/2506: Loss 27.181976\n",
      "Batch 1719/2506: Loss 55.540947\n",
      "Batch 1720/2506: Loss 39.964035\n",
      "Batch 1721/2506: Loss 51.594921\n",
      "Batch 1722/2506: Loss 67.088089\n",
      "Batch 1723/2506: Loss 97.394119\n",
      "Batch 1724/2506: Loss 73.091042\n",
      "Batch 1725/2506: Loss 40.997318\n",
      "Batch 1726/2506: Loss 106.296745\n",
      "Batch 1727/2506: Loss 81.425430\n",
      "Batch 1728/2506: Loss 80.771858\n",
      "Batch 1729/2506: Loss 46.417191\n",
      "Batch 1730/2506: Loss 112.805801\n",
      "Batch 1731/2506: Loss 48.815514\n",
      "Batch 1732/2506: Loss 183.940704\n",
      "Batch 1733/2506: Loss 168.536652\n",
      "Batch 1734/2506: Loss 80.730804\n",
      "Batch 1735/2506: Loss 43.683151\n",
      "Batch 1736/2506: Loss 58.011547\n",
      "Batch 1737/2506: Loss 38.594208\n",
      "Batch 1738/2506: Loss 40.930511\n",
      "Batch 1739/2506: Loss 65.992195\n",
      "Batch 1740/2506: Loss 91.130386\n",
      "Batch 1741/2506: Loss 61.523243\n",
      "Batch 1742/2506: Loss 177.858200\n",
      "Batch 1743/2506: Loss 108.018425\n",
      "Batch 1744/2506: Loss 63.341187\n",
      "Batch 1745/2506: Loss 117.990768\n",
      "Batch 1746/2506: Loss 35.444374\n",
      "Batch 1747/2506: Loss 61.074448\n",
      "Batch 1748/2506: Loss 146.440186\n",
      "Batch 1749/2506: Loss 44.129333\n",
      "Batch 1750/2506: Loss 66.420113\n",
      "Batch 1751/2506: Loss 43.081936\n",
      "Batch 1752/2506: Loss 66.174255\n",
      "Batch 1753/2506: Loss 94.376648\n",
      "Batch 1754/2506: Loss 67.006279\n",
      "Batch 1755/2506: Loss 54.608791\n",
      "Batch 1756/2506: Loss 45.009323\n",
      "Batch 1757/2506: Loss 155.277893\n",
      "Batch 1758/2506: Loss 43.503601\n",
      "Batch 1759/2506: Loss 48.871815\n",
      "Batch 1760/2506: Loss 48.875771\n",
      "Batch 1761/2506: Loss 89.480896\n",
      "Batch 1762/2506: Loss 43.172977\n",
      "Batch 1763/2506: Loss 55.099567\n",
      "Batch 1764/2506: Loss 55.434460\n",
      "Batch 1765/2506: Loss 100.814270\n",
      "Batch 1766/2506: Loss 88.294098\n",
      "Batch 1767/2506: Loss 86.124084\n",
      "Batch 1768/2506: Loss 78.373894\n",
      "Batch 1769/2506: Loss 26.428801\n",
      "Batch 1770/2506: Loss 157.144302\n",
      "Batch 1771/2506: Loss 104.781464\n",
      "Batch 1772/2506: Loss 64.557991\n",
      "Batch 1773/2506: Loss 103.376068\n",
      "Batch 1774/2506: Loss 85.576889\n",
      "Batch 1775/2506: Loss 117.579605\n",
      "Batch 1776/2506: Loss 79.630905\n",
      "Batch 1777/2506: Loss 45.518097\n",
      "Batch 1778/2506: Loss 86.359695\n",
      "Batch 1779/2506: Loss 59.946312\n",
      "Batch 1780/2506: Loss 102.179993\n",
      "Batch 1781/2506: Loss 91.878403\n",
      "Batch 1782/2506: Loss 41.102551\n",
      "Batch 1783/2506: Loss 47.613388\n",
      "Batch 1784/2506: Loss 62.681046\n",
      "Batch 1785/2506: Loss 39.125286\n",
      "Batch 1786/2506: Loss 73.192398\n",
      "Batch 1787/2506: Loss 41.163261\n",
      "Batch 1788/2506: Loss 73.809258\n",
      "Batch 1789/2506: Loss 92.240410\n",
      "Batch 1790/2506: Loss 63.374813\n",
      "Batch 1791/2506: Loss 63.750244\n",
      "Batch 1792/2506: Loss 102.030777\n",
      "Batch 1793/2506: Loss 102.992447\n",
      "Batch 1794/2506: Loss 60.990730\n",
      "Batch 1795/2506: Loss 103.645401\n",
      "Batch 1796/2506: Loss 90.916428\n",
      "Batch 1797/2506: Loss 42.156769\n",
      "Batch 1798/2506: Loss 122.783707\n",
      "Batch 1799/2506: Loss 40.314125\n",
      "Batch 1800/2506: Loss 53.666603\n",
      "Batch 1801/2506: Loss 174.305832\n",
      "Batch 1802/2506: Loss 38.838085\n",
      "Batch 1803/2506: Loss 30.160116\n",
      "Batch 1804/2506: Loss 81.718628\n",
      "Batch 1805/2506: Loss 39.707985\n",
      "Batch 1806/2506: Loss 111.888245\n",
      "Batch 1807/2506: Loss 101.517197\n",
      "Batch 1808/2506: Loss 58.213238\n",
      "Batch 1809/2506: Loss 67.059288\n",
      "Batch 1810/2506: Loss 40.432549\n",
      "Batch 1811/2506: Loss 106.599319\n",
      "Batch 1812/2506: Loss 61.683670\n",
      "Batch 1813/2506: Loss 87.304909\n",
      "Batch 1814/2506: Loss 55.070797\n",
      "Batch 1815/2506: Loss 160.204132\n",
      "Batch 1816/2506: Loss 122.721436\n",
      "Batch 1817/2506: Loss 149.851196\n",
      "Batch 1818/2506: Loss 45.710251\n",
      "Batch 1819/2506: Loss 114.385307\n",
      "Batch 1820/2506: Loss 42.245121\n",
      "Batch 1821/2506: Loss 79.864319\n",
      "Batch 1822/2506: Loss 115.769684\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1823/2506: Loss 55.091042\n",
      "Batch 1824/2506: Loss 61.435768\n",
      "Batch 1825/2506: Loss 105.772324\n",
      "Batch 1826/2506: Loss 40.023872\n",
      "Batch 1827/2506: Loss 136.691086\n",
      "Batch 1828/2506: Loss 54.049042\n",
      "Batch 1829/2506: Loss 125.686539\n",
      "Batch 1830/2506: Loss 140.376541\n",
      "Batch 1831/2506: Loss 89.380081\n",
      "Batch 1832/2506: Loss 38.238808\n",
      "Batch 1833/2506: Loss 73.636269\n",
      "Batch 1834/2506: Loss 120.680092\n",
      "Batch 1835/2506: Loss 86.533661\n",
      "Batch 1836/2506: Loss 38.117607\n",
      "Batch 1837/2506: Loss 85.424324\n",
      "Batch 1838/2506: Loss 162.822723\n",
      "Batch 1839/2506: Loss 121.589973\n",
      "Batch 1840/2506: Loss 159.474304\n",
      "Batch 1841/2506: Loss 135.052521\n",
      "Batch 1842/2506: Loss 111.034317\n",
      "Batch 1843/2506: Loss 103.688904\n",
      "Batch 1844/2506: Loss 58.844078\n",
      "Batch 1845/2506: Loss 109.865852\n",
      "Batch 1846/2506: Loss 83.301666\n",
      "Batch 1847/2506: Loss 80.151749\n",
      "Batch 1848/2506: Loss 109.911926\n",
      "Batch 1849/2506: Loss 54.679665\n",
      "Batch 1850/2506: Loss 79.761902\n",
      "Batch 1851/2506: Loss 127.971863\n",
      "Batch 1852/2506: Loss 109.134193\n",
      "Batch 1853/2506: Loss 42.933517\n",
      "Batch 1854/2506: Loss 83.746452\n",
      "Batch 1855/2506: Loss 55.331039\n",
      "Batch 1856/2506: Loss 73.687798\n",
      "Batch 1857/2506: Loss 55.561680\n",
      "Batch 1858/2506: Loss 70.151260\n",
      "Batch 1859/2506: Loss 92.466431\n",
      "Batch 1860/2506: Loss 66.657646\n",
      "Batch 1861/2506: Loss 55.265110\n",
      "Batch 1862/2506: Loss 104.936905\n",
      "Batch 1863/2506: Loss 32.387180\n",
      "Batch 1864/2506: Loss 48.785061\n",
      "Batch 1865/2506: Loss 77.418854\n",
      "Batch 1866/2506: Loss 49.399612\n",
      "Batch 1867/2506: Loss 30.695259\n",
      "Batch 1868/2506: Loss 79.735352\n",
      "Batch 1869/2506: Loss 60.121559\n",
      "Batch 1870/2506: Loss 181.903442\n",
      "Batch 1871/2506: Loss 78.422432\n",
      "Batch 1872/2506: Loss 90.394867\n",
      "Batch 1873/2506: Loss 205.035812\n",
      "Batch 1874/2506: Loss 69.242683\n",
      "Batch 1875/2506: Loss 101.060684\n",
      "Batch 1876/2506: Loss 110.737503\n",
      "Batch 1877/2506: Loss 35.207905\n",
      "Batch 1878/2506: Loss 68.825806\n",
      "Batch 1879/2506: Loss 61.946659\n",
      "Batch 1880/2506: Loss 78.064240\n",
      "Batch 1881/2506: Loss 92.105881\n",
      "Batch 1882/2506: Loss 54.815926\n",
      "Batch 1883/2506: Loss 67.306046\n",
      "Batch 1884/2506: Loss 54.023354\n",
      "Batch 1885/2506: Loss 156.363297\n",
      "Batch 1886/2506: Loss 31.294842\n",
      "Batch 1887/2506: Loss 280.844086\n",
      "Batch 1888/2506: Loss 153.646439\n",
      "Batch 1889/2506: Loss 43.958328\n",
      "Batch 1890/2506: Loss 112.753029\n",
      "Batch 1891/2506: Loss 59.604786\n",
      "Batch 1892/2506: Loss 97.042511\n",
      "Batch 1893/2506: Loss 74.651451\n",
      "Batch 1894/2506: Loss 71.454697\n",
      "Batch 1895/2506: Loss 77.037666\n",
      "Batch 1896/2506: Loss 45.142017\n",
      "Batch 1897/2506: Loss 49.343781\n",
      "Batch 1898/2506: Loss 54.183220\n",
      "Batch 1899/2506: Loss 30.855942\n",
      "Batch 1900/2506: Loss 122.367775\n",
      "Batch 1901/2506: Loss 94.606201\n",
      "Batch 1902/2506: Loss 58.746166\n",
      "Batch 1903/2506: Loss 71.463493\n",
      "Batch 1904/2506: Loss 46.132919\n",
      "Batch 1905/2506: Loss 49.763065\n",
      "Batch 1906/2506: Loss 78.680511\n",
      "Batch 1907/2506: Loss 126.194977\n",
      "Batch 1908/2506: Loss 61.757149\n",
      "Batch 1909/2506: Loss 118.290276\n",
      "Batch 1910/2506: Loss 173.198654\n",
      "Batch 1911/2506: Loss 151.356308\n",
      "Batch 1912/2506: Loss 124.502457\n",
      "Batch 1913/2506: Loss 61.776695\n",
      "Batch 1914/2506: Loss 70.032623\n",
      "Batch 1915/2506: Loss 58.150005\n",
      "Batch 1916/2506: Loss 59.859783\n",
      "Batch 1917/2506: Loss 54.700035\n",
      "Batch 1918/2506: Loss 73.619980\n",
      "Batch 1919/2506: Loss 90.645660\n",
      "Batch 1920/2506: Loss 139.103546\n",
      "Batch 1921/2506: Loss 124.237518\n",
      "Batch 1922/2506: Loss 79.297173\n",
      "Batch 1923/2506: Loss 49.820801\n",
      "Batch 1924/2506: Loss 80.419296\n",
      "Batch 1925/2506: Loss 49.412060\n",
      "Batch 1926/2506: Loss 59.050186\n",
      "Batch 1927/2506: Loss 138.573578\n",
      "Batch 1928/2506: Loss 155.343048\n",
      "Batch 1929/2506: Loss 70.573822\n",
      "Batch 1930/2506: Loss 106.348900\n",
      "Batch 1931/2506: Loss 60.281693\n",
      "Batch 1932/2506: Loss 77.885544\n",
      "Batch 1933/2506: Loss 118.495857\n",
      "Batch 1934/2506: Loss 118.678391\n",
      "Batch 1935/2506: Loss 71.825584\n",
      "Batch 1936/2506: Loss 94.075577\n",
      "Batch 1937/2506: Loss 41.568230\n",
      "Batch 1938/2506: Loss 85.888115\n",
      "Batch 1939/2506: Loss 50.741051\n",
      "Batch 1940/2506: Loss 37.653679\n",
      "Batch 1941/2506: Loss 81.164474\n",
      "Batch 1942/2506: Loss 81.406853\n",
      "Batch 1943/2506: Loss 103.081764\n",
      "Batch 1944/2506: Loss 71.659592\n",
      "Batch 1945/2506: Loss 51.889412\n",
      "Batch 1946/2506: Loss 91.600945\n",
      "Batch 1947/2506: Loss 52.337929\n",
      "Batch 1948/2506: Loss 149.047379\n",
      "Batch 1949/2506: Loss 133.153061\n",
      "Batch 1950/2506: Loss 40.045422\n",
      "Batch 1951/2506: Loss 44.815376\n",
      "Batch 1952/2506: Loss 36.953403\n",
      "Batch 1953/2506: Loss 36.468109\n",
      "Batch 1954/2506: Loss 104.265625\n",
      "Batch 1955/2506: Loss 40.427990\n",
      "Batch 1956/2506: Loss 197.893646\n",
      "Batch 1957/2506: Loss 40.620483\n",
      "Batch 1958/2506: Loss 83.883530\n",
      "Batch 1959/2506: Loss 61.649353\n",
      "Batch 1960/2506: Loss 106.772545\n",
      "Batch 1961/2506: Loss 73.311607\n",
      "Batch 1962/2506: Loss 59.408066\n",
      "Batch 1963/2506: Loss 96.437782\n",
      "Batch 1964/2506: Loss 54.507446\n",
      "Batch 1965/2506: Loss 67.941353\n",
      "Batch 1966/2506: Loss 174.558167\n",
      "Batch 1967/2506: Loss 66.020493\n",
      "Batch 1968/2506: Loss 57.183762\n",
      "Batch 1969/2506: Loss 62.483868\n",
      "Batch 1970/2506: Loss 33.288818\n",
      "Batch 1971/2506: Loss 47.310200\n",
      "Batch 1972/2506: Loss 109.501862\n",
      "Batch 1973/2506: Loss 55.478413\n",
      "Batch 1974/2506: Loss 65.103912\n",
      "Batch 1975/2506: Loss 55.321800\n",
      "Batch 1976/2506: Loss 67.468826\n",
      "Batch 1977/2506: Loss 63.204971\n",
      "Batch 1978/2506: Loss 88.518257\n",
      "Batch 1979/2506: Loss 75.480042\n",
      "Batch 1980/2506: Loss 51.886490\n",
      "Batch 1981/2506: Loss 124.064430\n",
      "Batch 1982/2506: Loss 27.995188\n",
      "Batch 1983/2506: Loss 78.879166\n",
      "Batch 1984/2506: Loss 125.982285\n",
      "Batch 1985/2506: Loss 39.277851\n",
      "Batch 1986/2506: Loss 97.647087\n",
      "Batch 1987/2506: Loss 44.468128\n",
      "Batch 1988/2506: Loss 57.560360\n",
      "Batch 1989/2506: Loss 45.748878\n",
      "Batch 1990/2506: Loss 40.436718\n",
      "Batch 1991/2506: Loss 55.618919\n",
      "Batch 1992/2506: Loss 94.321915\n",
      "Batch 1993/2506: Loss 77.105545\n",
      "Batch 1994/2506: Loss 92.737404\n",
      "Batch 1995/2506: Loss 111.044418\n",
      "Batch 1996/2506: Loss 48.285988\n",
      "Batch 1997/2506: Loss 31.521227\n",
      "Batch 1998/2506: Loss 38.060242\n",
      "Batch 1999/2506: Loss 53.347916\n",
      "Batch 2000/2506: Loss 70.371651\n",
      "Batch 2001/2506: Loss 75.930328\n",
      "Batch 2002/2506: Loss 125.088806\n",
      "Batch 2003/2506: Loss 128.308960\n",
      "Batch 2004/2506: Loss 32.728298\n",
      "Batch 2005/2506: Loss 64.998436\n",
      "Batch 2006/2506: Loss 30.698565\n",
      "Batch 2007/2506: Loss 38.959526\n",
      "Batch 2008/2506: Loss 35.947578\n",
      "Batch 2009/2506: Loss 116.183823\n",
      "Batch 2010/2506: Loss 83.305466\n",
      "Batch 2011/2506: Loss 98.781654\n",
      "Batch 2012/2506: Loss 90.431633\n",
      "Batch 2013/2506: Loss 101.754379\n",
      "Batch 2014/2506: Loss 64.195999\n",
      "Batch 2015/2506: Loss 103.431618\n",
      "Batch 2016/2506: Loss 93.466110\n",
      "Batch 2017/2506: Loss 69.044144\n",
      "Batch 2018/2506: Loss 50.841190\n",
      "Batch 2019/2506: Loss 57.502853\n",
      "Batch 2020/2506: Loss 57.136196\n",
      "Batch 2021/2506: Loss 65.924812\n",
      "Batch 2022/2506: Loss 75.499245\n",
      "Batch 2023/2506: Loss 72.263779\n",
      "Batch 2024/2506: Loss 33.933010\n",
      "Batch 2025/2506: Loss 40.359192\n",
      "Batch 2026/2506: Loss 94.520256\n",
      "Batch 2027/2506: Loss 62.720802\n",
      "Batch 2028/2506: Loss 134.856079\n",
      "Batch 2029/2506: Loss 56.876949\n",
      "Batch 2030/2506: Loss 74.174988\n",
      "Batch 2031/2506: Loss 51.594269\n",
      "Batch 2032/2506: Loss 65.275093\n",
      "Batch 2033/2506: Loss 76.135933\n",
      "Batch 2034/2506: Loss 69.281059\n",
      "Batch 2035/2506: Loss 108.963379\n",
      "Batch 2036/2506: Loss 135.813049\n",
      "Batch 2037/2506: Loss 53.452003\n",
      "Batch 2038/2506: Loss 89.650696\n",
      "Batch 2039/2506: Loss 52.565071\n",
      "Batch 2040/2506: Loss 80.787811\n",
      "Batch 2041/2506: Loss 27.264526\n",
      "Batch 2042/2506: Loss 82.393967\n",
      "Batch 2043/2506: Loss 53.102974\n",
      "Batch 2044/2506: Loss 38.499081\n",
      "Batch 2045/2506: Loss 39.968365\n",
      "Batch 2046/2506: Loss 125.265457\n",
      "Batch 2047/2506: Loss 38.355904\n",
      "Batch 2048/2506: Loss 42.896446\n",
      "Batch 2049/2506: Loss 68.797066\n",
      "Batch 2050/2506: Loss 66.927391\n",
      "Batch 2051/2506: Loss 87.629059\n",
      "Batch 2052/2506: Loss 73.520622\n",
      "Batch 2053/2506: Loss 93.328415\n",
      "Batch 2054/2506: Loss 130.594208\n",
      "Batch 2055/2506: Loss 41.432091\n",
      "Batch 2056/2506: Loss 71.018906\n",
      "Batch 2057/2506: Loss 57.717712\n",
      "Batch 2058/2506: Loss 67.862495\n",
      "Batch 2059/2506: Loss 58.757160\n",
      "Batch 2060/2506: Loss 84.009857\n",
      "Batch 2061/2506: Loss 78.982384\n",
      "Batch 2062/2506: Loss 73.496994\n",
      "Batch 2063/2506: Loss 59.249237\n",
      "Batch 2064/2506: Loss 149.117279\n",
      "Batch 2065/2506: Loss 88.809784\n",
      "Batch 2066/2506: Loss 109.406082\n",
      "Batch 2067/2506: Loss 183.785004\n",
      "Batch 2068/2506: Loss 64.907204\n",
      "Batch 2069/2506: Loss 68.992210\n",
      "Batch 2070/2506: Loss 35.354683\n",
      "Batch 2071/2506: Loss 78.629044\n",
      "Batch 2072/2506: Loss 95.415619\n",
      "Batch 2073/2506: Loss 99.526062\n",
      "Batch 2074/2506: Loss 73.749443\n",
      "Batch 2075/2506: Loss 46.150661\n",
      "Batch 2076/2506: Loss 56.101513\n",
      "Batch 2077/2506: Loss 90.168167\n",
      "Batch 2078/2506: Loss 112.131393\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2079/2506: Loss 111.495682\n",
      "Batch 2080/2506: Loss 69.165451\n",
      "Batch 2081/2506: Loss 53.962414\n",
      "Batch 2082/2506: Loss 89.924637\n",
      "Batch 2083/2506: Loss 77.201462\n",
      "Batch 2084/2506: Loss 56.029152\n",
      "Batch 2085/2506: Loss 126.141617\n",
      "Batch 2086/2506: Loss 77.470879\n",
      "Batch 2087/2506: Loss 51.621681\n",
      "Batch 2088/2506: Loss 70.764687\n",
      "Batch 2089/2506: Loss 101.282387\n",
      "Batch 2090/2506: Loss 147.266785\n",
      "Batch 2091/2506: Loss 50.393978\n",
      "Batch 2092/2506: Loss 34.331596\n",
      "Batch 2093/2506: Loss 76.892403\n",
      "Batch 2094/2506: Loss 103.520958\n",
      "Batch 2095/2506: Loss 38.303982\n",
      "Batch 2096/2506: Loss 54.193504\n",
      "Batch 2097/2506: Loss 72.840714\n",
      "Batch 2098/2506: Loss 40.086628\n",
      "Batch 2099/2506: Loss 122.906708\n",
      "Batch 2100/2506: Loss 100.181335\n",
      "Batch 2101/2506: Loss 124.931259\n",
      "Batch 2102/2506: Loss 39.852417\n",
      "Batch 2103/2506: Loss 151.879578\n",
      "Batch 2104/2506: Loss 26.738544\n",
      "Batch 2105/2506: Loss 69.543694\n",
      "Batch 2106/2506: Loss 39.030861\n",
      "Batch 2107/2506: Loss 70.509026\n",
      "Batch 2108/2506: Loss 116.902191\n",
      "Batch 2109/2506: Loss 70.034966\n",
      "Batch 2110/2506: Loss 32.262268\n",
      "Batch 2111/2506: Loss 63.579197\n",
      "Batch 2112/2506: Loss 36.845058\n",
      "Batch 2113/2506: Loss 128.350555\n",
      "Batch 2114/2506: Loss 43.156876\n",
      "Batch 2115/2506: Loss 90.816360\n",
      "Batch 2116/2506: Loss 68.720825\n",
      "Batch 2117/2506: Loss 50.945065\n",
      "Batch 2118/2506: Loss 62.768993\n",
      "Batch 2119/2506: Loss 50.780209\n",
      "Batch 2120/2506: Loss 87.300674\n",
      "Batch 2121/2506: Loss 80.735275\n",
      "Batch 2122/2506: Loss 56.586914\n",
      "Batch 2123/2506: Loss 72.098358\n",
      "Batch 2124/2506: Loss 44.869194\n",
      "Batch 2125/2506: Loss 64.361748\n",
      "Batch 2126/2506: Loss 89.441002\n",
      "Batch 2127/2506: Loss 79.243889\n",
      "Batch 2128/2506: Loss 49.730919\n",
      "Batch 2129/2506: Loss 29.337317\n",
      "Batch 2130/2506: Loss 197.941605\n",
      "Batch 2131/2506: Loss 60.797581\n",
      "Batch 2132/2506: Loss 125.265022\n",
      "Batch 2133/2506: Loss 66.065575\n",
      "Batch 2134/2506: Loss 62.625168\n",
      "Batch 2135/2506: Loss 77.499466\n",
      "Batch 2136/2506: Loss 54.221500\n",
      "Batch 2137/2506: Loss 116.153862\n",
      "Batch 2138/2506: Loss 82.380196\n",
      "Batch 2139/2506: Loss 183.470825\n",
      "Batch 2140/2506: Loss 133.433823\n",
      "Batch 2141/2506: Loss 93.778961\n",
      "Batch 2142/2506: Loss 90.574997\n",
      "Batch 2143/2506: Loss 60.513371\n",
      "Batch 2144/2506: Loss 47.420486\n",
      "Batch 2145/2506: Loss 52.741470\n",
      "Batch 2146/2506: Loss 71.185165\n",
      "Batch 2147/2506: Loss 77.581955\n",
      "Batch 2148/2506: Loss 53.880711\n",
      "Batch 2149/2506: Loss 49.762096\n",
      "Batch 2150/2506: Loss 50.198269\n",
      "Batch 2151/2506: Loss 65.887238\n",
      "Batch 2152/2506: Loss 99.589630\n",
      "Batch 2153/2506: Loss 36.575356\n",
      "Batch 2154/2506: Loss 39.085289\n",
      "Batch 2155/2506: Loss 50.565186\n",
      "Batch 2156/2506: Loss 104.753510\n",
      "Batch 2157/2506: Loss 71.378845\n",
      "Batch 2158/2506: Loss 130.828140\n",
      "Batch 2159/2506: Loss 128.465500\n",
      "Batch 2160/2506: Loss 36.170670\n",
      "Batch 2161/2506: Loss 79.851074\n",
      "Batch 2162/2506: Loss 58.508499\n",
      "Batch 2163/2506: Loss 53.088444\n",
      "Batch 2164/2506: Loss 83.639435\n",
      "Batch 2165/2506: Loss 41.899605\n",
      "Batch 2166/2506: Loss 109.536285\n",
      "Batch 2167/2506: Loss 85.802864\n",
      "Batch 2168/2506: Loss 41.545288\n",
      "Batch 2169/2506: Loss 36.369942\n",
      "Batch 2170/2506: Loss 53.459023\n",
      "Batch 2171/2506: Loss 90.567177\n",
      "Batch 2172/2506: Loss 158.763092\n",
      "Batch 2173/2506: Loss 144.410614\n",
      "Batch 2174/2506: Loss 163.994873\n",
      "Batch 2175/2506: Loss 60.955410\n",
      "Batch 2176/2506: Loss 42.822353\n",
      "Batch 2177/2506: Loss 112.665253\n",
      "Batch 2178/2506: Loss 152.190689\n",
      "Batch 2179/2506: Loss 152.260223\n",
      "Batch 2180/2506: Loss 102.327560\n",
      "Batch 2181/2506: Loss 99.061996\n",
      "Batch 2182/2506: Loss 191.138062\n",
      "Batch 2183/2506: Loss 104.246292\n",
      "Batch 2184/2506: Loss 160.495331\n",
      "Batch 2185/2506: Loss 69.539566\n",
      "Batch 2186/2506: Loss 112.184059\n",
      "Batch 2187/2506: Loss 130.911911\n",
      "Batch 2188/2506: Loss 58.682480\n",
      "Batch 2189/2506: Loss 87.682777\n",
      "Batch 2190/2506: Loss 33.115776\n",
      "Batch 2191/2506: Loss 28.504028\n",
      "Batch 2192/2506: Loss 79.860329\n",
      "Batch 2193/2506: Loss 103.425125\n",
      "Batch 2194/2506: Loss 55.943405\n",
      "Batch 2195/2506: Loss 163.775360\n",
      "Batch 2196/2506: Loss 93.770386\n",
      "Batch 2197/2506: Loss 42.157616\n",
      "Batch 2198/2506: Loss 89.576942\n",
      "Batch 2199/2506: Loss 53.088547\n",
      "Batch 2200/2506: Loss 112.891724\n",
      "Batch 2201/2506: Loss 29.717550\n",
      "Batch 2202/2506: Loss 70.999596\n",
      "Batch 2203/2506: Loss 115.402939\n",
      "Batch 2204/2506: Loss 67.891960\n",
      "Batch 2205/2506: Loss 78.817444\n",
      "Batch 2206/2506: Loss 69.088127\n",
      "Batch 2207/2506: Loss 40.654587\n",
      "Batch 2208/2506: Loss 87.747940\n",
      "Batch 2209/2506: Loss 46.323338\n",
      "Batch 2210/2506: Loss 75.176880\n",
      "Batch 2211/2506: Loss 47.349648\n",
      "Batch 2212/2506: Loss 70.458458\n",
      "Batch 2213/2506: Loss 56.985306\n",
      "Batch 2214/2506: Loss 110.069199\n",
      "Batch 2215/2506: Loss 61.355011\n",
      "Batch 2216/2506: Loss 114.637360\n",
      "Batch 2217/2506: Loss 85.296539\n",
      "Batch 2218/2506: Loss 55.347084\n",
      "Batch 2219/2506: Loss 31.265419\n",
      "Batch 2220/2506: Loss 34.570587\n",
      "Batch 2221/2506: Loss 64.345474\n",
      "Batch 2222/2506: Loss 49.688408\n",
      "Batch 2223/2506: Loss 41.015129\n",
      "Batch 2224/2506: Loss 106.312813\n",
      "Batch 2225/2506: Loss 49.761539\n",
      "Batch 2226/2506: Loss 98.877678\n",
      "Batch 2227/2506: Loss 66.441757\n",
      "Batch 2228/2506: Loss 52.391537\n",
      "Batch 2229/2506: Loss 62.223106\n",
      "Batch 2230/2506: Loss 109.943359\n",
      "Batch 2231/2506: Loss 142.023773\n",
      "Batch 2232/2506: Loss 56.094788\n",
      "Batch 2233/2506: Loss 151.618500\n",
      "Batch 2234/2506: Loss 77.669670\n",
      "Batch 2235/2506: Loss 67.157166\n",
      "Batch 2236/2506: Loss 45.481850\n",
      "Batch 2237/2506: Loss 53.195496\n",
      "Batch 2238/2506: Loss 43.440712\n",
      "Batch 2239/2506: Loss 49.324875\n",
      "Batch 2240/2506: Loss 46.070171\n",
      "Batch 2241/2506: Loss 60.832859\n",
      "Batch 2242/2506: Loss 170.885101\n",
      "Batch 2243/2506: Loss 75.242447\n",
      "Batch 2244/2506: Loss 103.321251\n",
      "Batch 2245/2506: Loss 100.880020\n",
      "Batch 2246/2506: Loss 32.549294\n",
      "Batch 2247/2506: Loss 62.159943\n",
      "Batch 2248/2506: Loss 71.102036\n",
      "Batch 2249/2506: Loss 77.942070\n",
      "Batch 2250/2506: Loss 69.922226\n",
      "Batch 2251/2506: Loss 89.287056\n",
      "Batch 2252/2506: Loss 67.095978\n",
      "Batch 2253/2506: Loss 34.592464\n",
      "Batch 2254/2506: Loss 94.371414\n",
      "Batch 2255/2506: Loss 166.213226\n",
      "Batch 2256/2506: Loss 30.939928\n",
      "Batch 2257/2506: Loss 133.939758\n",
      "Batch 2258/2506: Loss 65.806122\n",
      "Batch 2259/2506: Loss 56.983200\n",
      "Batch 2260/2506: Loss 53.063629\n",
      "Batch 2261/2506: Loss 166.840454\n",
      "Batch 2262/2506: Loss 43.888008\n",
      "Batch 2263/2506: Loss 64.734886\n",
      "Batch 2264/2506: Loss 93.866165\n",
      "Batch 2265/2506: Loss 42.386307\n",
      "Batch 2266/2506: Loss 45.534206\n",
      "Batch 2267/2506: Loss 57.517899\n",
      "Batch 2268/2506: Loss 118.602982\n",
      "Batch 2269/2506: Loss 54.308266\n",
      "Batch 2270/2506: Loss 109.794121\n",
      "Batch 2271/2506: Loss 101.053734\n",
      "Batch 2272/2506: Loss 232.954361\n",
      "Batch 2273/2506: Loss 43.536629\n",
      "Batch 2274/2506: Loss 92.565231\n",
      "Batch 2275/2506: Loss 96.186783\n",
      "Batch 2276/2506: Loss 83.591652\n",
      "Batch 2277/2506: Loss 68.694389\n",
      "Batch 2278/2506: Loss 80.132851\n",
      "Batch 2279/2506: Loss 124.418961\n",
      "Batch 2280/2506: Loss 56.037910\n",
      "Batch 2281/2506: Loss 65.452126\n",
      "Batch 2282/2506: Loss 79.806831\n",
      "Batch 2283/2506: Loss 126.209259\n",
      "Batch 2284/2506: Loss 59.252632\n",
      "Batch 2285/2506: Loss 108.067703\n",
      "Batch 2286/2506: Loss 56.375477\n",
      "Batch 2287/2506: Loss 101.494453\n",
      "Batch 2288/2506: Loss 43.296883\n",
      "Batch 2289/2506: Loss 171.015137\n",
      "Batch 2290/2506: Loss 178.483490\n",
      "Batch 2291/2506: Loss 83.448097\n",
      "Batch 2292/2506: Loss 66.836502\n",
      "Batch 2293/2506: Loss 47.490940\n",
      "Batch 2294/2506: Loss 113.801727\n",
      "Batch 2295/2506: Loss 121.104065\n",
      "Batch 2296/2506: Loss 28.426268\n",
      "Batch 2297/2506: Loss 38.531784\n",
      "Batch 2298/2506: Loss 174.092468\n",
      "Batch 2299/2506: Loss 122.275558\n",
      "Batch 2300/2506: Loss 86.069717\n",
      "Batch 2301/2506: Loss 32.598003\n",
      "Batch 2302/2506: Loss 53.130966\n",
      "Batch 2303/2506: Loss 53.990303\n",
      "Batch 2304/2506: Loss 95.623802\n",
      "Batch 2305/2506: Loss 69.037796\n",
      "Batch 2306/2506: Loss 38.110142\n",
      "Batch 2307/2506: Loss 71.097000\n",
      "Batch 2308/2506: Loss 66.660019\n",
      "Batch 2309/2506: Loss 71.935005\n",
      "Batch 2310/2506: Loss 84.118690\n",
      "Batch 2311/2506: Loss 37.703346\n",
      "Batch 2312/2506: Loss 48.121468\n",
      "Batch 2313/2506: Loss 69.333115\n",
      "Batch 2314/2506: Loss 78.889114\n",
      "Batch 2315/2506: Loss 34.706924\n",
      "Batch 2316/2506: Loss 67.926437\n",
      "Batch 2317/2506: Loss 49.055122\n",
      "Batch 2318/2506: Loss 65.872375\n",
      "Batch 2319/2506: Loss 51.053055\n",
      "Batch 2320/2506: Loss 85.424530\n",
      "Batch 2321/2506: Loss 50.065910\n",
      "Batch 2322/2506: Loss 49.714027\n",
      "Batch 2323/2506: Loss 44.507866\n",
      "Batch 2324/2506: Loss 91.907043\n",
      "Batch 2325/2506: Loss 60.898315\n",
      "Batch 2326/2506: Loss 49.730949\n",
      "Batch 2327/2506: Loss 59.945724\n",
      "Batch 2328/2506: Loss 101.681168\n",
      "Batch 2329/2506: Loss 97.420227\n",
      "Batch 2330/2506: Loss 70.530426\n",
      "Batch 2331/2506: Loss 42.302361\n",
      "Batch 2332/2506: Loss 36.023014\n",
      "Batch 2333/2506: Loss 58.975868\n",
      "Batch 2334/2506: Loss 73.340210\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2335/2506: Loss 49.304031\n",
      "Batch 2336/2506: Loss 67.583694\n",
      "Batch 2337/2506: Loss 42.157417\n",
      "Batch 2338/2506: Loss 35.328903\n",
      "Batch 2339/2506: Loss 49.144886\n",
      "Batch 2340/2506: Loss 97.293556\n",
      "Batch 2341/2506: Loss 41.243385\n",
      "Batch 2342/2506: Loss 27.932610\n",
      "Batch 2343/2506: Loss 106.997833\n",
      "Batch 2344/2506: Loss 45.324356\n",
      "Batch 2345/2506: Loss 52.483662\n",
      "Batch 2346/2506: Loss 99.057076\n",
      "Batch 2347/2506: Loss 48.579624\n",
      "Batch 2348/2506: Loss 68.477257\n",
      "Batch 2349/2506: Loss 51.953674\n",
      "Batch 2350/2506: Loss 34.600552\n",
      "Batch 2351/2506: Loss 118.114349\n",
      "Batch 2352/2506: Loss 149.179535\n",
      "Batch 2353/2506: Loss 53.738556\n",
      "Batch 2354/2506: Loss 77.055862\n",
      "Batch 2355/2506: Loss 35.332268\n",
      "Batch 2356/2506: Loss 78.516953\n",
      "Batch 2357/2506: Loss 87.203110\n",
      "Batch 2358/2506: Loss 49.967655\n",
      "Batch 2359/2506: Loss 44.818932\n",
      "Batch 2360/2506: Loss 138.761551\n",
      "Batch 2361/2506: Loss 28.640545\n",
      "Batch 2362/2506: Loss 190.725464\n",
      "Batch 2363/2506: Loss 86.637680\n",
      "Batch 2364/2506: Loss 37.897121\n",
      "Batch 2365/2506: Loss 120.134399\n",
      "Batch 2366/2506: Loss 51.231339\n",
      "Batch 2367/2506: Loss 60.614304\n",
      "Batch 2368/2506: Loss 43.237431\n",
      "Batch 2369/2506: Loss 91.551155\n",
      "Batch 2370/2506: Loss 65.006836\n",
      "Batch 2371/2506: Loss 149.470062\n",
      "Batch 2372/2506: Loss 75.417175\n",
      "Batch 2373/2506: Loss 80.021317\n",
      "Batch 2374/2506: Loss 110.405815\n",
      "Batch 2375/2506: Loss 59.861252\n",
      "Batch 2376/2506: Loss 73.146545\n",
      "Batch 2377/2506: Loss 50.312046\n",
      "Batch 2378/2506: Loss 63.122772\n",
      "Batch 2379/2506: Loss 77.732460\n",
      "Batch 2380/2506: Loss 98.161819\n",
      "Batch 2381/2506: Loss 30.792393\n",
      "Batch 2382/2506: Loss 59.701267\n",
      "Batch 2383/2506: Loss 48.792484\n",
      "Batch 2384/2506: Loss 177.815857\n",
      "Batch 2385/2506: Loss 51.694984\n",
      "Batch 2386/2506: Loss 78.844421\n",
      "Batch 2387/2506: Loss 47.980473\n",
      "Batch 2388/2506: Loss 83.705597\n",
      "Batch 2389/2506: Loss 45.772923\n",
      "Batch 2390/2506: Loss 127.484100\n",
      "Batch 2391/2506: Loss 62.879478\n",
      "Batch 2392/2506: Loss 33.847328\n",
      "Batch 2393/2506: Loss 72.153824\n",
      "Batch 2394/2506: Loss 49.637573\n",
      "Batch 2395/2506: Loss 78.321045\n",
      "Batch 2396/2506: Loss 72.801910\n",
      "Batch 2397/2506: Loss 67.841614\n",
      "Batch 2398/2506: Loss 54.627010\n",
      "Batch 2399/2506: Loss 41.963699\n",
      "Batch 2400/2506: Loss 79.147430\n",
      "Batch 2401/2506: Loss 75.402473\n",
      "Batch 2402/2506: Loss 43.428551\n",
      "Batch 2403/2506: Loss 83.919724\n",
      "Batch 2404/2506: Loss 48.292259\n",
      "Batch 2405/2506: Loss 77.790894\n",
      "Batch 2406/2506: Loss 63.132225\n",
      "Batch 2407/2506: Loss 63.824642\n",
      "Batch 2408/2506: Loss 73.499847\n",
      "Batch 2409/2506: Loss 61.377407\n",
      "Batch 2410/2506: Loss 70.936539\n",
      "Batch 2411/2506: Loss 62.019321\n",
      "Batch 2412/2506: Loss 51.169247\n",
      "Batch 2413/2506: Loss 82.202660\n",
      "Batch 2414/2506: Loss 40.795925\n",
      "Batch 2415/2506: Loss 116.200020\n",
      "Batch 2416/2506: Loss 70.825272\n",
      "Batch 2417/2506: Loss 137.545410\n",
      "Batch 2418/2506: Loss 43.625984\n",
      "Batch 2419/2506: Loss 78.419205\n",
      "Batch 2420/2506: Loss 30.232189\n",
      "Batch 2421/2506: Loss 143.629028\n",
      "Batch 2422/2506: Loss 62.305130\n",
      "Batch 2423/2506: Loss 59.412937\n",
      "Batch 2424/2506: Loss 131.274490\n",
      "Batch 2425/2506: Loss 146.613297\n",
      "Batch 2426/2506: Loss 62.736687\n",
      "Batch 2427/2506: Loss 158.430969\n",
      "Batch 2428/2506: Loss 42.139290\n",
      "Batch 2429/2506: Loss 89.994072\n",
      "Batch 2430/2506: Loss 38.949169\n",
      "Batch 2431/2506: Loss 38.490032\n",
      "Batch 2432/2506: Loss 36.464226\n",
      "Batch 2433/2506: Loss 97.344833\n",
      "Batch 2434/2506: Loss 110.214401\n",
      "Batch 2435/2506: Loss 40.113747\n",
      "Batch 2436/2506: Loss 74.008270\n",
      "Batch 2437/2506: Loss 38.213264\n",
      "Batch 2438/2506: Loss 102.963120\n",
      "Batch 2439/2506: Loss 94.813225\n",
      "Batch 2440/2506: Loss 128.576828\n",
      "Batch 2441/2506: Loss 85.230499\n",
      "Batch 2442/2506: Loss 31.131887\n",
      "Batch 2443/2506: Loss 38.387856\n",
      "Batch 2444/2506: Loss 89.187981\n",
      "Batch 2445/2506: Loss 71.005035\n",
      "Batch 2446/2506: Loss 84.026733\n",
      "Batch 2447/2506: Loss 51.566597\n",
      "Batch 2448/2506: Loss 44.120773\n",
      "Batch 2449/2506: Loss 97.342361\n",
      "Batch 2450/2506: Loss 75.059380\n",
      "Batch 2451/2506: Loss 144.046722\n",
      "Batch 2452/2506: Loss 67.818512\n",
      "Batch 2453/2506: Loss 37.343552\n",
      "Batch 2454/2506: Loss 86.965836\n",
      "Batch 2455/2506: Loss 99.573692\n",
      "Batch 2456/2506: Loss 74.927521\n",
      "Batch 2457/2506: Loss 113.138535\n",
      "Batch 2458/2506: Loss 73.906654\n",
      "Batch 2459/2506: Loss 122.655190\n",
      "Batch 2460/2506: Loss 78.669670\n",
      "Batch 2461/2506: Loss 49.578743\n",
      "Batch 2462/2506: Loss 43.329060\n",
      "Batch 2463/2506: Loss 54.632187\n",
      "Batch 2464/2506: Loss 69.289246\n",
      "Batch 2465/2506: Loss 75.915573\n",
      "Batch 2466/2506: Loss 153.121475\n",
      "Batch 2467/2506: Loss 44.350243\n",
      "Batch 2468/2506: Loss 117.282562\n",
      "Batch 2469/2506: Loss 78.887367\n",
      "Batch 2470/2506: Loss 72.151878\n",
      "Batch 2471/2506: Loss 212.308319\n",
      "Batch 2472/2506: Loss 74.475990\n",
      "Batch 2473/2506: Loss 96.136078\n",
      "Batch 2474/2506: Loss 59.440430\n",
      "Batch 2475/2506: Loss 68.581871\n",
      "Batch 2476/2506: Loss 31.053694\n",
      "Batch 2477/2506: Loss 71.708214\n",
      "Batch 2478/2506: Loss 31.497101\n",
      "Batch 2479/2506: Loss 86.627510\n",
      "Batch 2480/2506: Loss 53.314598\n",
      "Batch 2481/2506: Loss 96.792374\n",
      "Batch 2482/2506: Loss 29.538597\n",
      "Batch 2483/2506: Loss 82.341255\n",
      "Batch 2484/2506: Loss 75.778534\n",
      "Batch 2485/2506: Loss 58.930428\n",
      "Batch 2486/2506: Loss 42.053139\n",
      "Batch 2487/2506: Loss 56.507290\n",
      "Batch 2488/2506: Loss 76.000900\n",
      "Batch 2489/2506: Loss 46.957455\n",
      "Batch 2490/2506: Loss 146.508362\n",
      "Batch 2491/2506: Loss 106.539787\n",
      "Batch 2492/2506: Loss 32.164719\n",
      "Batch 2493/2506: Loss 83.175812\n",
      "Batch 2494/2506: Loss 40.346432\n",
      "Batch 2495/2506: Loss 66.749977\n",
      "Batch 2496/2506: Loss 99.059509\n",
      "Batch 2497/2506: Loss 115.771553\n",
      "Batch 2498/2506: Loss 89.569405\n",
      "Batch 2499/2506: Loss 96.413345\n",
      "Batch 2500/2506: Loss 52.032280\n",
      "Batch 2501/2506: Loss 64.214645\n",
      "Batch 2502/2506: Loss 61.760071\n",
      "Batch 2503/2506: Loss 113.179474\n",
      "Batch 2504/2506: Loss 39.376236\n",
      "Batch 2505/2506: Loss 38.787857\n"
     ]
    }
   ],
   "source": [
    "import torchvision.transforms\n",
    "from torchvision.transforms import *\n",
    "\n",
    "train_dataset = VOCYolo(\n",
    "    annotator.labels,\n",
    "    annotations,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.Resize((448, 448)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            mean=[0.4547857, 0.4349471, 0.40525291],\n",
    "            std=[0.12003352, 0.12323549, 0.1392444]\n",
    "        )\n",
    "    ])\n",
    ")\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, shuffle=True, batch_size=2, num_workers=0, pin_memory=True)\n",
    "\n",
    "model = YOLOv1().float().cuda()\n",
    "\n",
    "criterion = YoloLoss(lambda_coord=5, lambda_noobj=0.5)\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "    \n",
    "for i, (image, label) in enumerate(train_dataloader):\n",
    "    with torch.cuda.amp.autocast():\n",
    "        image = image.cuda(non_blocking=True)\n",
    "        label = label.cuda(non_blocking=True)\n",
    "        \n",
    "    output = model(image)\n",
    "    loss = criterion(output, label)\n",
    "    scaler.scale(loss).backward()\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    \n",
    "    print(\"Batch %d/%d: Loss %.6f\" % (i, len(train_dataloader), loss))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
