{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38ca604",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "from torch.nn import *\n",
    "\n",
    "import torchvision\n",
    "from torchvision.transforms import *\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "from maintrainer.loss import YoloLoss\n",
    "from model import YOLOv1\n",
    "from maintrainer.dataset import VOCYOLOAnnotator, VOCYolo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd75e1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotator = VOCYOLOAnnotator(\n",
    "#     annotation_root=r'C:\\Dataset\\VOCdevkit\\VOC2008\\Annotations',\n",
    "#     image_root=r'C:\\Dataset\\VOCdevkit\\VOC2008\\JPEGImages'\n",
    "    annotation_root='/media/jungin500/windows-10/Dataset/VOCdevkit/VOC2008/Annotations',\n",
    "    image_root='/media/jungin500/windows-10/Dataset/VOCdevkit/VOC2008/JPEGImages'\n",
    ")\n",
    "\n",
    "annotations = annotator.parse_annotation()\n",
    "print(\"Annotation[0]:\", annotations[0][0])\n",
    "print(\"Annotation[1]:\", annotations[0][1])\n",
    "\n",
    "print(annotator.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e36b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from helper.drawer import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a61307",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLOv1().float().cuda()\n",
    "\n",
    "criterion = YoloLoss(lambda_coord=5, lambda_noobj=0.5)\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf3e720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torchvision.transforms\n",
    "# from torchvision.transforms import *\n",
    "# import albumentations as A\n",
    "\n",
    "# train_dataset = VOCYolo(\n",
    "#     annotator.labels,\n",
    "#     annotations,\n",
    "#     transform=transforms.Compose([\n",
    "# #         transforms.Resize((448, 448)),\n",
    "#         transforms.ToTensor(),\n",
    "# #         transforms.Normalize(\n",
    "# #             mean=[0.4547857, 0.4349471, 0.40525291],\n",
    "# #             std=[0.12003352, 0.12323549, 0.1392444]\n",
    "# #         )\n",
    "#     ]),\n",
    "#     augmentations=[\n",
    "#         A.PadIfNeeded(min_height=448, min_width=448),\n",
    "#         A.RandomCrop(width=448, height=448),\n",
    "#         A.HorizontalFlip(p=0.5),\n",
    "#         A.RandomBrightnessContrast(p=0.2)\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# train_dataloader = torch.utils.data.DataLoader(train_dataset, shuffle=True, batch_size=1, num_workers=0, pin_memory=True)\n",
    "\n",
    "## Only shows first entry label as it's WRONG!\n",
    "\n",
    "# bunch_of_batch = []\n",
    "# for i, item in enumerate(train_dataloader):\n",
    "#     if i >= 4:\n",
    "#         break\n",
    "#     bunch_of_batch.append(item)\n",
    "\n",
    "# fig = plt.figure(figsize=(16, 16))\n",
    "# fig.suptitle(\"GT Annotation based visualization\", fontsize=24)\n",
    "# for i, (image, label) in enumerate(bunch_of_batch):\n",
    "#     ax = fig.add_subplot(2, 2, i + 1)\n",
    "#     image = torch.squeeze(image)\n",
    "#     label = torch.squeeze(label)\n",
    "    \n",
    "#     image = (image.numpy().transpose((1, 2, 0)) * 255).astype(np.uint8)\n",
    "#     label = label.numpy()\n",
    "    \n",
    "#     image = Image.fromarray(image)\n",
    "#     image = draw_cell_boundaries(image)\n",
    "#     image = draw_center_cell_object(image, annotator, annotations[i][1])\n",
    "#     ax.imshow(np.array(image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8eacead",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import torchvision.transforms\n",
    "from torchvision.transforms import *\n",
    "import albumentations as A\n",
    "\n",
    "train_dataset = VOCYolo(\n",
    "    annotator.labels,\n",
    "    annotations,\n",
    "    transform=transforms.Compose([\n",
    "#         transforms.Resize((448, 448)),\n",
    "        transforms.ToTensor(),\n",
    "#         transforms.Normalize(\n",
    "#             mean=[0.4547857, 0.4349471, 0.40525291],\n",
    "#             std=[0.12003352, 0.12323549, 0.1392444]\n",
    "#         )\n",
    "    ]),\n",
    "    augmentations=[\n",
    "        A.PadIfNeeded(min_width=448, min_height=448),\n",
    "        A.RandomCrop(width=448, height=448),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.RandomBrightnessContrast(p=0.2)\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, shuffle=True, batch_size=1, num_workers=0, pin_memory=True)\n",
    "\n",
    "bunch_of_batch = []\n",
    "for i, item in enumerate(train_dataloader):\n",
    "    if i >= 4:\n",
    "        break\n",
    "    bunch_of_batch.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ceaa394",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16, 16))\n",
    "fig.suptitle(\"Label based visualization\", fontsize=24)\n",
    "for i, (image, label) in enumerate(bunch_of_batch):\n",
    "    ax = fig.add_subplot(2, 2, i + 1)\n",
    "    image = torch.squeeze(image)\n",
    "    label = torch.squeeze(label)\n",
    "    \n",
    "    image = (image.numpy().transpose((1, 2, 0)) * 255).astype(np.uint8)\n",
    "    label = label.numpy()\n",
    "    \n",
    "    image = Image.fromarray(image)\n",
    "    image = draw_cell_boundaries(image)\n",
    "    image = draw_center_cell_object_label(image, annotator, label)\n",
    "    ax.imshow(np.array(image))\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9970f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing over pseudo-output\n",
    "\n",
    "def do_visualization_pseudo(title = None):\n",
    "    fig = plt.figure(figsize=(16, 16))\n",
    "    fig.suptitle(\"Output based visualization\" if title is None else title, fontsize=24)\n",
    "    \n",
    "    model.eval()\n",
    "    for i, (image, label) in enumerate(bunch_of_batch):\n",
    "        ax = fig.add_subplot(2, 2, i + 1)\n",
    "        output = torch.zeros((1, 30, 7, 7))\n",
    "#         output[0, :, 3, 3] = torch.from_numpy(np.array([\n",
    "#             #x, y, w, h, c, classes\n",
    "#             0.0, 0.0, 1.38629, 0.40547, 0.6, # 0.5, 0.5, 0.8, 0.6, 0.6,\n",
    "#             -2.197224, 2.197225, -0.405465, -1.3862894, 0.1, # 0.1, 0.9, 0.4, 0.2, 0.1,\n",
    "#             # ['aeroplane', 'bicycle', 'bird', 'boat', 'bottle',\n",
    "#             #  'bus', 'car', 'cat', 'chair', 'cow',\n",
    "#             #  'diningtable', 'dog', 'horse', 'motorbike', 'person',\n",
    "#             #  'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor']\n",
    "#             0.0, 0.0, 0.0, 0.0, 0.0,\n",
    "#             0.0, 0.0, 0.0, 0.0, 0.0,\n",
    "#             0.0, 0.0, 0.0, 0.0, 0.0,\n",
    "#             0.0, 0.0, 1.0, 0.0, 0.0\n",
    "#         ]))\n",
    "        for y in range(7):\n",
    "            for x in range(7):\n",
    "                output[0, :, y, x] = torch.from_numpy(np.array([\n",
    "                    #x, y, w, h, c, classes\n",
    "                    0.0, 0.0, -2.1, -2.1, 0.84729,  # 0.5, 0.5, 0.13, 0.13, 0.7\n",
    "                    0.0, 0.0, -2.1, -2.1, 0.84729,  # 0.5, 0.5, 0.13, 0.13, 0.7\n",
    "                    # ['aeroplane', 'bicycle', 'bird', 'boat', 'bottle',\n",
    "                    #  'bus', 'car', 'cat', 'chair', 'cow',\n",
    "                    #  'diningtable', 'dog', 'horse', 'motorbike', 'person',\n",
    "                    #  'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor']\n",
    "                    0.0, 0.0, 0.0, 0.0, 0.0,\n",
    "                    0.0, 0.0, 0.0, 0.0, 0.0,\n",
    "                    0.0, 0.0, 0.0, 0.0, 0.0,\n",
    "                    0.0, 0.0, 1.0, 0.0, 0.0\n",
    "                ]))\n",
    "\n",
    "        image = torch.squeeze(image.cpu())\n",
    "        output = torch.squeeze(output)\n",
    "\n",
    "        image = (image.numpy().transpose((1, 2, 0)) * 255).astype(np.uint8)\n",
    "        output = output.numpy()\n",
    "\n",
    "        image = Image.fromarray(image)\n",
    "        image = draw_cell_boundaries(image)\n",
    "        image = draw_center_cell_object_output(image, annotator, output, confidence_threshold=0.51)\n",
    "        ax.imshow(np.array(image))\n",
    "    model.train()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "do_visualization_pseudo()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
